# Diagramme de Cas d'Utilisation - Route POST /api/v1/sites/analyze

## Vue d'ensemble

Ce document pr√©sente les cas d'utilisation (Use Cases) de la route `POST /api/v1/sites/analyze` avec un diagramme UML et une description d√©taill√©e de chaque cas d'utilisation.

## Diagramme de Cas d'Utilisation

```mermaid
graph TB
    subgraph "Acteurs Externes"
        CLIENT[üë§ Client<br/>Utilisateur/Application]
        TARGET[üåê Site Web Cible<br/>Target Website]
        OLLAMA[ü§ñ Ollama<br/>LLM Server]
        QDRANT[üíæ Qdrant<br/>Vector Store]
    end
    
    subgraph "Syst√®me - API Editorial"
        API[üì° API Router<br/>/api/v1/sites/analyze]
        ORCH[üéØ Orchestrator<br/>EditorialAnalysisOrchestrator]
        AGENT_ANALYSIS[üß† Analysis Agent<br/>Multi-LLM]
        AGENT_SCRAPE[üì• Scraping Agent<br/>Enhanced]
        SITEMAP[üó∫Ô∏è Sitemap Discovery]
        CRAWLER[üï∑Ô∏è Page Crawler]
    end
    
    subgraph "Cas d'Utilisation Principaux"
        UC1[UC1: Lancer Analyse<br/>√âditoriale]
        UC2[UC2: D√©couvrir URLs<br/>via Sitemap]
        UC3[UC3: Crawler Pages<br/>Web]
        UC4[UC4: Analyser Contenu<br/>Multi-LLM]
        UC5[UC5: Cr√©er/Mettre √† jour<br/>Profil √âditorial]
        UC6[UC6: Scraper Site Client<br/>Automatiquement]
        UC7[UC7: G√©rer Erreurs<br/>et Exceptions]
        UC8[UC8: Suivre Progression<br/>Execution]
    end
    
    subgraph "Cas d'Utilisation Secondaires"
        UC2A[UC2.1: Parser Sitemap<br/>XML]
        UC2B[UC2.2: Fallback<br/>Homepage]
        UC3A[UC3.1: V√©rifier<br/>robots.txt]
        UC3B[UC3.2: Extraire<br/>Contenu HTML]
        UC4A[UC4.1: Analyser avec<br/>Llama3]
        UC4B[UC4.2: Analyser avec<br/>Mistral]
        UC4C[UC4.3: Analyser avec<br/>Phi3]
        UC4D[UC4.4: Synth√©tiser<br/>R√©sultats]
        UC6A[UC6.1: D√©couvrir Articles<br/>Multi-sources]
        UC6B[UC6.2: Scorer URLs<br/>Probabilit√©]
        UC6C[UC6.3: Extraire Articles<br/>Adaptatif]
        UC6D[UC6.4: Indexer dans<br/>Qdrant]
    end
    
    CLIENT -->|1. POST request| UC1
    UC1 -->|2. D√©clenche| API
    API -->|3. Orchestre| ORCH
    
    ORCH -->|4.1| UC2
    UC2 -->|4.1.1| UC2A
    UC2 -->|4.1.2| UC2B
    UC2 -->|4.1.3| SITEMAP
    SITEMAP -->|Fetch| TARGET
    
    ORCH -->|4.2| UC3
    UC3 -->|4.2.1| UC3A
    UC3 -->|4.2.2| UC3B
    UC3 -->|4.2.3| CRAWLER
    CRAWLER -->|Fetch| TARGET
    
    ORCH -->|4.3| UC4
    UC4 -->|4.3.1| UC4A
    UC4 -->|4.3.2| UC4B
    UC4 -->|4.3.3| UC4C
    UC4 -->|4.3.4| UC4D
    UC4 -->|Utilise| AGENT_ANALYSIS
    AGENT_ANALYSIS -->|Appelle| OLLAMA
    
    ORCH -->|4.4| UC5
    UC5 -->|Persiste| API
    
    ORCH -->|4.5| UC6
    UC6 -->|5.1| UC6A
    UC6 -->|5.2| UC6B
    UC6 -->|5.3| UC6C
    UC6 -->|5.4| UC6D
    UC6 -->|Utilise| AGENT_SCRAPE
    UC6D -->|Indexe| QDRANT
    
    UC1 -.->|En cas d'erreur| UC7
    UC2 -.->|En cas d'erreur| UC7
    UC3 -.->|En cas d'erreur| UC7
    UC4 -.->|En cas d'erreur| UC7
    UC5 -.->|En cas d'erreur| UC7
    UC6 -.->|En cas d'erreur| UC7
    
    CLIENT -->|Polling| UC8
    UC8 -->|Lit| API
    
    style CLIENT fill:#4a90e2,color:#fff
    style UC1 fill:#50c878,color:#fff
    style UC4 fill:#ff6b6b,color:#fff
    style UC6 fill:#f39c12,color:#fff
    style UC7 fill:#e74c3c,color:#fff
    style OLLAMA fill:#3498db,color:#fff
    style QDRANT fill:#9b59b6,color:#fff
```

## Description D√©taill√©e des Cas d'Utilisation

### UC1: Lancer Analyse √âditoriale

**Acteur Principal** : Client (Utilisateur/Application)

**Pr√©conditions** :
- Le client a acc√®s √† l'API
- Le domaine cible est valide et accessible

**Flux Principal** :
1. Le client envoie une requ√™te `POST /api/v1/sites/analyze` avec `{domain, max_pages}`
2. L'API cr√©e un enregistrement `workflow_execution` avec status "pending"
3. L'API lance une t√¢che en arri√®re-plan
4. L'API retourne imm√©diatement `202 Accepted` avec `execution_id`
5. Le workflow s'ex√©cute en arri√®re-plan

**Flux Alternatif 1A** : Domaine invalide
- L'API retourne `400 Bad Request` avec message d'erreur

**Flux Alternatif 1B** : Erreur serveur
- L'API retourne `500 Internal Server Error`
- L'ex√©cution est marqu√©e comme "failed"

**Postconditions** :
- Un `workflow_execution` est cr√©√©
- Le workflow est lanc√© en arri√®re-plan
- Le client peut suivre la progression via `execution_id`

---

### UC2: D√©couvrir URLs via Sitemap

**Acteur Principal** : Orchestrator

**Pr√©conditions** :
- Le domaine est valide
- L'orchestrator est initialis√©

**Flux Principal** :
1. L'orchestrator appelle `get_sitemap_urls(domain)`
2. Le syst√®me tente de d√©couvrir le sitemap XML
3. Le syst√®me parse le sitemap et extrait les URLs
4. Les URLs sont retourn√©es √† l'orchestrator

**Flux Alternatif 2A** : Sitemap non trouv√©
- Le syst√®me utilise la homepage comme fallback
- Retourne `[f"https://{domain}"]`

**Flux Alternatif 2B** : Sitemap invalide
- Le syst√®me utilise la homepage comme fallback

**Postconditions** :
- Une liste d'URLs est disponible pour le crawl
- Au moins une URL (homepage) est disponible

---

### UC3: Crawler Pages Web

**Acteur Principal** : Orchestrator

**Pr√©conditions** :
- Une liste d'URLs est disponible
- Le crawler est initialis√©

**Flux Principal** :
1. L'orchestrator appelle `crawl_multiple_pages(urls)`
2. Pour chaque URL :
   - V√©rification basique de robots.txt
   - Fetch de la page via httpx
   - Extraction du contenu HTML
   - Extraction du texte brut
   - Extraction des m√©tadonn√©es (titre, description)
3. Les pages crawl√©ees sont retourn√©es

**Flux Alternatif 3A** : Page inaccessible
- La page est marqu√©e comme "failed"
- Le workflow continue avec les autres pages

**Flux Alternatif 3B** : Aucune page crawl√©ee
- Le workflow √©choue avec erreur "No pages crawled"

**Postconditions** :
- Une liste de pages crawl√©ees est disponible
- Chaque page contient : HTML, texte, m√©tadonn√©es

---

### UC4: Analyser Contenu Multi-LLM

**Acteur Principal** : Analysis Agent

**Pr√©conditions** :
- Le contenu combin√© est disponible
- Les mod√®les LLM sont accessibles (Ollama)

**Flux Principal** :
1. L'agent combine le contenu de toutes les pages
2. L'agent analyse avec **Llama3:8b** (analyse principale)
3. L'agent analyse avec **Mistral:7b** (analyse compl√©mentaire)
4. L'agent analyse avec **Phi3:medium** (analyse d√©taill√©e)
5. L'agent synth√©tise les 3 analyses avec Llama3
6. Les r√©sultats sont retourn√©s

**Flux Alternatif 4A** : √âchec d'un mod√®le LLM
- L'agent continue avec les autres mod√®les
- Utilise un merge manuel si la synth√®se √©choue

**Flux Alternatif 4B** : Tous les mod√®les √©chouent
- Le workflow √©choue avec erreur LLM

**Postconditions** :
- Un profil √©ditorial complet est g√©n√©r√©
- Contient : language_level, editorial_tone, target_audience, activity_domains, content_structure, keywords, style_features

---

### UC5: Cr√©er/Mettre √† jour Profil √âditorial

**Acteur Principal** : Orchestrator

**Pr√©conditions** :
- Les r√©sultats de l'analyse LLM sont disponibles
- La session base de donn√©es est active

**Flux Principal** :
1. L'orchestrator v√©rifie si un profil existe pour le domaine
2. Si le profil n'existe pas :
   - Cr√©ation d'un nouveau `site_profile`
3. Si le profil existe :
   - Mise √† jour du profil existant
4. Sauvegarde des r√©sultats dans `site_analysis_result`
5. Mise √† jour de `workflow_execution` avec status "completed"

**Flux Alternatif 5A** : Erreur de sauvegarde
- Le workflow √©choue
- L'ex√©cution est marqu√©e comme "failed"

**Postconditions** :
- Un `site_profile` existe pour le domaine
- Un `site_analysis_result` est cr√©√©
- Le `workflow_execution` est mis √† jour

---

### UC6: Scraper Site Client Automatiquement

**Acteur Principal** : Scraping Agent (Background)

**Pr√©conditions** :
- L'analyse principale est termin√©e
- Le `site_profile` existe

**Flux Principal** :
1. L'orchestrator lance le scraping en arri√®re-plan
2. **Phase Discovery** : D√©couverte d'articles via API, RSS, Sitemap, Heuristiques
3. **Phase Scoring** : Calcul de probabilit√© pour chaque URL
4. **Phase Extraction** : Extraction adaptative du contenu
5. **Phase Validation** : Validation de la qualit√© du contenu
6. Sauvegarde des articles dans `client_articles`
7. Indexation dans Qdrant (collection `{domain}_client_articles`)

**Flux Alternatif 6A** : Erreur de scraping
- L'erreur est logg√©e
- Le workflow principal n'est pas interrompu
- Les articles d√©j√† scrap√©s sont sauvegard√©s

**Postconditions** :
- Des articles clients sont sauvegard√©s (si disponibles)
- Un profil de d√©couverte est cr√©√©/mis √† jour
- Les articles sont index√©s dans Qdrant

---

### UC7: G√©rer Erreurs et Exceptions

**Acteur Principal** : Syst√®me (Tous les composants)

**Pr√©conditions** :
- Une erreur survient dans le workflow

**Flux Principal** :
1. L'erreur est captur√©e
2. L'erreur est logg√©e avec contexte
3. Le `workflow_execution` est mis √† jour avec :
   - `status` = "failed"
   - `error_message` = message d'erreur
   - `was_success` = false
4. L'exception est propag√©e si n√©cessaire

**Flux Alternatif 7A** : Erreur non critique (scraping)
- L'erreur est logg√©e
- Le workflow continue

**Postconditions** :
- L'erreur est trac√©e
- Le statut de l'ex√©cution refl√®te l'√©chec

---

### UC8: Suivre Progression Execution

**Acteur Principal** : Client

**Pr√©conditions** :
- Un `execution_id` existe
- L'ex√©cution est en cours ou termin√©e

**Flux Principal** :
1. Le client fait une requ√™te `GET /api/v1/executions/{execution_id}`
2. L'API lit le `workflow_execution` depuis la base
3. L'API retourne le statut actuel :
   - "pending" : En attente
   - "running" : En cours
   - "completed" : Termin√© avec succ√®s
   - "failed" : √âchou√©

**Flux Alternatif 8A** : Execution non trouv√©e
- L'API retourne `404 Not Found`

**Postconditions** :
- Le client conna√Æt le statut de l'ex√©cution
- Le client peut acc√©der aux r√©sultats si termin√©

---

## Relations entre Cas d'Utilisation

### Inclusion (<<include>>)
- **UC1** inclut **UC2**, **UC3**, **UC4**, **UC5**
- **UC4** inclut **UC4.1**, **UC4.2**, **UC4.3**, **UC4.4**
- **UC6** inclut **UC6.1**, **UC6.2**, **UC6.3**, **UC6.4**

### Extension (<<extend>>)
- **UC7** √©tend tous les cas d'utilisation principaux (gestion d'erreurs)
- **UC2.2** √©tend **UC2** (fallback homepage)
- **UC8** peut √™tre utilis√© ind√©pendamment pour suivre n'importe quelle ex√©cution

### G√©n√©ralisation
- **UC2.1**, **UC2.2** sont des sp√©cialisations de **UC2**
- **UC3.1**, **UC3.2** sont des sp√©cialisations de **UC3**
- **UC4.1**, **UC4.2**, **UC4.3**, **UC4.4** sont des sp√©cialisations de **UC4**
- **UC6.1**, **UC6.2**, **UC6.3**, **UC6.4** sont des sp√©cialisations de **UC6**

## Matrice des Acteurs et Cas d'Utilisation

| Acteur | UC1 | UC2 | UC3 | UC4 | UC5 | UC6 | UC7 | UC8 |
|--------|-----|-----|-----|-----|-----|-----|-----|-----|
| Client | ‚úÖ Initiateur | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ Lecteur |
| Orchestrator | ‚úÖ Ex√©cute | ‚úÖ Ex√©cute | ‚úÖ Ex√©cute | ‚úÖ Orchestre | ‚úÖ Ex√©cute | ‚úÖ Lance | ‚úÖ G√®re | ‚ùå |
| Analysis Agent | ‚ùå | ‚ùå | ‚ùå | ‚úÖ Ex√©cute | ‚ùå | ‚ùå | ‚úÖ G√®re | ‚ùå |
| Scraping Agent | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ Ex√©cute | ‚úÖ G√®re | ‚ùå |
| Ollama | ‚ùå | ‚ùå | ‚ùå | ‚úÖ Utilis√© | ‚ùå | ‚ùå | ‚ùå | ‚ùå |
| Qdrant | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ Utilis√© | ‚ùå | ‚ùå |
| Target Website | ‚ùå | ‚úÖ Consult√© | ‚úÖ Consult√© | ‚ùå | ‚ùå | ‚úÖ Consult√© | ‚ùå | ‚ùå |

## Sc√©narios d'Utilisation

### Sc√©nario 1 : Analyse Standard R√©ussie

**Acteur** : Responsable Marketing

**Objectif** : Analyser le style √©ditorial de son site

**Pr√©conditions** : Site accessible avec 50+ pages

**Flux** :
1. ‚úÖ UC1 : Lance l'analyse
2. ‚úÖ UC2 : D√©couvre 50 URLs via sitemap
3. ‚úÖ UC3 : Crawle 50 pages avec succ√®s
4. ‚úÖ UC4 : Analyse avec 3 LLMs et synth√©tise
5. ‚úÖ UC5 : Cr√©e le profil √©ditorial
6. ‚úÖ UC6 : Scrape automatiquement 20 articles
7. ‚úÖ UC8 : Consulte les r√©sultats

**R√©sultat** : Profil √©ditorial complet disponible

---

### Sc√©nario 2 : Site avec Contenu Limit√©

**Acteur** : Consultant SEO

**Objectif** : Analyser un petit site (< 10 pages)

**Pr√©conditions** : Site avec peu de contenu

**Flux** :
1. ‚úÖ UC1 : Lance l'analyse
2. ‚úÖ UC2 : D√©couvre 8 URLs (UC2.2 : fallback homepage)
3. ‚úÖ UC3 : Crawle 8 pages
4. ‚úÖ UC4 : Analyse avec 3 LLMs
5. ‚úÖ UC5 : Cr√©e le profil (partiel)
6. ‚ö†Ô∏è UC6 : Scraping limit√© (peu d'articles trouv√©s)
7. ‚úÖ UC8 : Consulte les r√©sultats

**R√©sultat** : Profil √©ditorial partiel (avertissement sur contenu limit√©)

---

### Sc√©nario 3 : Erreur lors du Crawl

**Acteur** : D√©veloppeur

**Objectif** : Analyser un site prot√©g√©

**Pr√©conditions** : Site avec robots.txt restrictif

**Flux** :
1. ‚úÖ UC1 : Lance l'analyse
2. ‚úÖ UC2 : D√©couvre URLs
3. ‚ùå UC3 : √âchec du crawl (robots.txt bloque)
4. ‚úÖ UC7 : G√®re l'erreur
5. ‚úÖ UC8 : Consulte le statut "failed"

**R√©sultat** : Erreur explicite avec message

---

### Sc√©nario 4 : Analyse avec Scraping √âchou√©

**Acteur** : Agence Digitale

**Objectif** : Analyser un site, scraping optionnel

**Pr√©conditions** : Site analysable, scraping peut √©chouer

**Flux** :
1. ‚úÖ UC1 : Lance l'analyse
2. ‚úÖ UC2-UC5 : Analyse principale r√©ussie
3. ‚ùå UC6 : Scraping √©choue (site prot√©g√©)
4. ‚úÖ UC7 : Erreur de scraping logg√©e (non bloquante)
5. ‚úÖ UC8 : Consulte les r√©sultats (analyse OK, scraping √©chou√©)

**R√©sultat** : Profil √©ditorial disponible, scraping non effectu√©

## R√®gles M√©tier

1. **Respect robots.txt** : Le syst√®me respecte robots.txt (v√©rification basique)
2. **Limite de pages** : Maximum `max_pages` pages analys√©es (par d√©faut 50)
3. **Multi-LLM obligatoire** : Au moins 2 des 3 LLMs doivent r√©ussir
4. **Scraping non bloquant** : Les erreurs de scraping n'interrompent pas l'analyse principale
5. **Profil unique par domaine** : Un seul profil actif par domaine (mise √† jour si existe)
6. **Collection Qdrant dynamique** : Format `{domain}_client_articles` pour isolation

## Contraintes Techniques

- **Asynchrone** : Retour imm√©diat, traitement en arri√®re-plan
- **Timeout** : Pas de timeout explicite, mais gestion d'erreurs r√©seau
- **Concurrence** : Support de multiples analyses simultan√©es
- **Idempotence** : Relancer une analyse met √† jour le profil existant
















