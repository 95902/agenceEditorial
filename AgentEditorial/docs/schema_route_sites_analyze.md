# Sch√©ma de Base de Donn√©es Impact√©e - Route POST /api/v1/sites/analyze

## Vue d'ensemble

La route `POST /api/v1/sites/analyze` lance une analyse √©ditoriale compl√®te d'un domaine. Elle impacte plusieurs tables de la base de donn√©es lors de son ex√©cution.

## Flux d'ex√©cution

1. **Cr√©ation de l'ex√©cution** ‚Üí `workflow_executions`
2. **D√©couverte des URLs** (sitemap)
3. **Crawl des pages** ‚Üí (pas de cache utilis√©)
4. **Analyse LLM** ‚Üí `site_analysis_results`
5. **Cr√©ation/Mise √† jour du profil** ‚Üí `site_profiles`
6. **Scraping automatique du site client** ‚Üí `client_articles`, `site_discovery_profiles`, `url_discovery_scores`, `discovery_logs`

## Tables impact√©es

> ‚ö†Ô∏è **Note importante** : Les tables `scraping_permissions` et `crawl_cache` existent dans le sch√©ma mais **ne sont pas utilis√©es** dans ce workflow. Elles sont list√©es ci-dessous pour information mais ne sont pas impact√©es.

### 1. `workflow_executions` ‚≠ê **CRITIQUE**
- **Op√©ration** : CREATE, UPDATE
- **Description** : Enregistre l'ex√©cution du workflow d'analyse √©ditoriale
- **Champs impact√©s** :
  - `execution_id` (UUID, unique)
  - `workflow_type` = "editorial_analysis"
  - `status` : "pending" ‚Üí "running" ‚Üí "completed" ou "failed"
  - `input_data` : `{"domain": "...", "max_pages": ...}`
  - `output_data` : R√©sultats de l'analyse
  - `start_time`, `end_time`, `duration_seconds`
  - `was_success` : true/false
  - `error_message` : si √©chec

### 2. `site_profiles` ‚≠ê **CRITIQUE**
- **Op√©ration** : CREATE ou UPDATE
- **Description** : Profil √©ditorial du site analys√©
- **Champs impact√©s** :
  - `domain` (unique)
  - `analysis_date`
  - `language_level`
  - `editorial_tone`
  - `target_audience` (JSONB)
  - `activity_domains` (JSONB)
  - `content_structure` (JSONB)
  - `keywords` (JSONB)
  - `style_features` (JSONB)
  - `pages_analyzed`
  - `llm_models_used` (JSONB)

### 3. `site_analysis_results` ‚≠ê **CRITIQUE**
- **Op√©ration** : CREATE
- **Description** : R√©sultats d√©taill√©s de l'analyse par phase
- **Champs impact√©s** :
  - `site_profile_id` (FK ‚Üí `site_profiles.id`)
  - `execution_id` (FK ‚Üí `workflow_executions.execution_id`)
  - `analysis_phase` = "synthesis"
  - `phase_results` (JSONB) : R√©sultats complets de l'analyse LLM
  - `llm_model_used`
  - `processing_time_seconds`

### 4. `scraping_permissions` ‚ùå **NON UTILIS√âE**
- **Op√©ration** : Aucune
- **Description** : Cache des permissions robots.txt (table cr√©√©e mais non utilis√©e dans ce workflow)
- **Note** : 
  - La table existe et les fonctions CRUD existent dans `crud_permissions.py`
  - La fonction `parse_robots_txt()` utilise bien le cache, mais elle n'est **pas appel√©e** dans le workflow d'analyse
  - Le workflow utilise `check_robots_txt()` qui ne prend pas de `db_session` et n'utilise pas le cache
  - **Cette table n'est donc pas impact√©e par cette route**

### 5. `crawl_cache` ‚ùå **NON UTILIS√âE**
- **Op√©ration** : Aucune
- **Description** : Cache des pages crawl√©ees (table cr√©√©e mais non impl√©ment√©e)
- **Note** :
  - La table existe dans le mod√®le mais **aucune fonction CRUD n'existe**
  - Le param√®tre `check_cache` dans `crawl_page_async()` est marqu√© comme "not used, kept for API compatibility"
  - **Cette table n'est donc pas impact√©e par cette route**

### 6. `client_articles` üìù **SCRAPING AUTOMATIQUE**
- **Op√©ration** : CREATE
- **Description** : Articles du site client scrap√©s automatiquement apr√®s l'analyse
- **Champs impact√©s** :
  - `site_profile_id` (FK ‚Üí `site_profiles.id`)
  - `url` (unique)
  - `url_hash`
  - `title`
  - `author`
  - `published_date`
  - `content_text`
  - `content_html`
  - `word_count`
  - `keywords` (JSONB)
  - `article_metadata` (JSONB)
  - `qdrant_point_id` (si indexation Qdrant activ√©e)
  - `topic_id` (si topic modeling effectu√©)



## Diagramme de relations

```
workflow_executions (1)
    ‚îÇ
    ‚îú‚îÄ‚îÄ> site_analysis_results (N)
    ‚îÇ
site_profiles (1)
    ‚îÇ
    ‚îú‚îÄ‚îÄ> site_analysis_results (N)
    ‚îÇ
    ‚îú‚îÄ‚îÄ> client_articles (N) [via scraping automatique]
    ‚îÇ


‚ùå scraping_permissions (non utilis√©e dans ce workflow)
‚ùå crawl_cache (non utilis√©e dans ce workflow)
```

## Ordre d'impact

1. **Phase initiale** :
   - `workflow_executions` (CREATE)

2. **Phase crawl** :
   - (Aucune table de cache utilis√©e - les fonctions de cache ne sont pas appel√©es)

3. **Phase analyse** :
   - `site_profiles` (READ ou CREATE)
   - `site_analysis_results` (CREATE)
   - `workflow_executions` (UPDATE)

4. **Phase scraping automatique** (optionnel, en arri√®re-plan) :
   - `site_discovery_profiles` (CREATE ou UPDATE)
   - `url_discovery_scores` (CREATE ou UPDATE)
   - `client_articles` (CREATE)
   - `discovery_logs` (CREATE)

## Notes importantes

- ‚≠ê **CRITIQUE** : Tables essentielles pour le fonctionnement de la route
- ‚ùå **NON UTILIS√âES** : Tables cr√©√©es mais non utilis√©es dans ce workflow
- üìù **SCRAPING AUTOMATIQUE** : Tables cr√©√©es lors du scraping automatique du site client (√©tape 9 du workflow)

- Le scraping automatique est lanc√© en arri√®re-plan et ne bloque pas la r√©ponse de l'API
- Les erreurs de scraping n'interrompent pas le workflow principal
- ‚ö†Ô∏è **Les tables `scraping_permissions` et `crawl_cache` ne sont pas utilis√©es** : 
  - `scraping_permissions` : Les fonctions CRUD existent mais `parse_robots_txt()` n'est pas appel√©e dans le workflow
  - `crawl_cache` : Aucune fonction CRUD n'existe, le param√®tre `check_cache` est ignor√©

## Contraintes de cl√©s √©trang√®res

- `site_analysis_results.site_profile_id` ‚Üí `site_profiles.id` (CASCADE)
- `site_analysis_results.execution_id` ‚Üí `workflow_executions.execution_id` (CASCADE)
- `client_articles.site_profile_id` ‚Üí `site_profiles.id`
- `url_discovery_scores.domain` ‚Üí `site_discovery_profiles.domain` (logique, pas FK)









