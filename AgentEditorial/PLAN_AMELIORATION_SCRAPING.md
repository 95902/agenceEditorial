# Plan d'am√©lioration du scraping d'articles

## Date : 2025-12-02
## Objectif : Am√©liorer le taux de r√©cup√©ration d'articles (actuellement 0%)

---

## üìã Vue d'ensemble

Ce plan propose des am√©liorations progressives pour r√©soudre les probl√®mes identifi√©s dans `ANALYSIS_SCRAPING_ISSUES.md`. Les modifications sont organis√©es par priorit√© et impact estim√©.

**Impact attendu :** Passage de 0% √† 60-80% d'articles r√©cup√©r√©s

---

## üéØ Phase 1 : Corrections critiques (Priorit√© 1)

### T√¢che 1.1 : √âlargir les patterns de d√©couverte d'articles

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Modifications :**
- [ ] Cr√©er une constante `ARTICLE_URL_PATTERNS` avec tous les patterns fran√ßais
- [ ] Ajouter les patterns manquants identifi√©s dans l'analyse
- [ ] Ajouter des patterns g√©n√©riques (dates WordPress, slugs SEO)

**Patterns √† ajouter :**
```python
ARTICLE_URL_PATTERNS = [
    # Patterns existants
    r"/blog/",
    r"/article/",
    r"/actualites/",
    r"/news/",
    r"/posts/",
    r"/post/",
    
    # Nouveaux patterns fran√ßais
    r"/actualite/",           # Singulier
    r"/actu/",
    r"/articles/",            # Pluriel
    r"/communiques?/",        # Communiqu√©(s)
    r"/presse/",
    r"/notre-actu/",
    r"/media/",
    r"/publications?/",        # Publication(s)
    r"/ressources/",
    r"/conseils/",
    r"/guides/",
    r"/tutoriels/",
    r"/veille/",
    r"/insights/",
    r"/etudes/",
    r"/cas-client/",
    r"/cas-clients/",
    r"/temoignage/",
    r"/temoignages/",
    r"/whitepaper/",
    r"/livre-blanc/",
    r"/webinaire/",
    r"/webinaires/",
    
    # Patterns g√©n√©riques
    r"/\d{4}/\d{2}/",         # WordPress date pattern (YYYY/MM/)
    r"/\d{4}/\d{2}/\d{2}/",   # WordPress date pattern (YYYY/MM/DD/)
    r"/[-a-z0-9]+/",          # Slug SEO (format: /mon-article-seo/)
]
```

**Emplacement :**
- Ligne 75-83 : `discover_article_urls()` - Strategy 1 (Sitemap)
- Ligne 253-259 : `_discover_via_heuristics()` - Strategy 3

**Estimation :** 30 minutes

---

### T√¢che 1.2 : Ajouter un fallback intelligent pour le sitemap

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Probl√®me :** Si aucun pattern ne correspond, on retourne une liste vide.

**Solution :**
- [ ] Si aucun article trouv√© via patterns, prendre les N premi√®res URLs du sitemap
- [ ] Filtrer les URLs exclues (extensions, cat√©gories)
- [ ] Limiter √† `max_articles` m√™me en fallback

**Modifications :**
```python
# Apr√®s la boucle de filtrage par patterns (ligne 85-93)
if sitemap_count == 0 and len(sitemap_urls) > 0:
    # Fallback : prendre les premi√®res URLs du sitemap
    self.logger.info(
        "No articles found via patterns, using fallback",
        domain=domain,
        sitemap_urls_available=len(sitemap_urls),
    )
    
    # Exclure les extensions et cat√©gories
    excluded_extensions = ['.pdf', '.jpg', '.png', '.css', '.js', '.xml']
    excluded_patterns = [r'/category/', r'/tag/', r'/page/\d+']
    
    for url in sitemap_urls:
        if len(article_urls) >= max_articles:
            break
        
        # V√©rifier extensions
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            continue
        
        # V√©rifier cat√©gories/pagination
        if any(re.search(pattern, url, re.IGNORECASE) for pattern in excluded_patterns):
            continue
        
        article_urls.append(url)
        sitemap_count += 1
```

**Estimation :** 45 minutes

---

### T√¢che 1.3 : Am√©liorer la d√©tection de pages de cat√©gories

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Probl√®me :** Les pages de cat√©gories sont trait√©es comme des articles.

**Solution :**
- [ ] Cr√©er une m√©thode `is_category_page(url: str) -> bool`
- [ ] Exclure les pages de cat√©gories/pagination des r√©sultats
- [ ] Utiliser cette m√©thode dans le filtrage

**Nouvelle m√©thode :**
```python
def _is_category_page(self, url: str) -> bool:
    """
    D√©tecte les pages de cat√©gorie, tag ou pagination.
    
    Args:
        url: URL √† v√©rifier
        
    Returns:
        True si c'est une page de cat√©gorie/pagination
    """
    url_lower = url.lower()
    category_patterns = [
        r'/(category|tag|news|actualites?|blog)(/|$)',
        r'/page/\d+/?$',
        r'/\?paged=\d+',
        r'/\?page=\d+',
    ]
    return any(re.search(pattern, url_lower) for pattern in category_patterns)
```

**Utilisation :**
- Dans `discover_article_urls()` : exclure les cat√©gories du sitemap
- Dans `_discover_via_heuristics()` : exclure les cat√©gories des liens

**Estimation :** 30 minutes

---

## üéØ Phase 2 : Am√©liorations moyennes (Priorit√© 2)

### T√¢che 2.1 : Assouplir le filtrage par nombre de mots

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Modifications :**
- [ ] R√©duire `min_word_count` de 250 √† 150 mots
- [ ] Rendre cette valeur configurable via `__init__()`

**Changement :**
```python
def __init__(self, min_word_count: int = 150) -> None:
    """Initialize the scraping agent."""
    super().__init__("scraping")
    self.min_word_count = min_word_count  # Au lieu de 250
    self.max_age_days = 730  # 2 years
```

**Estimation :** 15 minutes

---

### T√¢che 2.2 : Am√©liorer les heuristics avec navigation r√©cursive

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Probl√®me :** Les heuristics ne cherchent que sur la homepage.

**Solution :**
- [ ] Impl√©menter une navigation r√©cursive vers les pages de blog
- [ ] Suivre les liens "Voir tous les articles", "Archives", etc.
- [ ] Limiter la profondeur de navigation (max_depth = 2-3)

**Nouvelle m√©thode :**
```python
async def _discover_via_heuristics(
    self,
    domain: str,
    max_urls: int,
    max_depth: int = 2,
) -> List[str]:
    """
    Discover article URLs via heuristics with recursive navigation.
    
    Args:
        domain: Domain name
        max_urls: Maximum URLs to discover
        max_depth: Maximum navigation depth
        
    Returns:
        List of article URLs
    """
    base_url = f"https://{domain}"
    article_urls = set()
    visited = set()
    
    # Pages de blog communes √† explorer
    blog_candidates = [
        f"{base_url}/blog/",
        f"{base_url}/actualites/",
        f"{base_url}/news/",
        f"{base_url}/articles/",
        f"{base_url}/ressources/",
    ]
    
    async def crawl_page(url: str, depth: int = 0):
        if depth > max_depth or url in visited or len(article_urls) >= max_urls:
            return
        
        visited.add(url)
        
        try:
            result = await crawl_page_async(url)
            if not result.get("success"):
                return
            
            html = result.get("html", "")
            soup = BeautifulSoup(html, "html.parser")
            
            # 1. D√©tecter les articles via balises <article>
            for article_tag in soup.find_all("article"):
                link = article_tag.find("a", href=True)
                if link:
                    href = link.get("href")
                    absolute_url = urljoin(base_url, href)
                    if self._is_article_url(absolute_url) and not self._is_category_page(absolute_url):
                        article_urls.add(absolute_url)
            
            # 2. D√©tecter les liens d'articles
            for link in soup.find_all("a", href=True):
                if len(article_urls) >= max_urls:
                    break
                
                href = link.get("href")
                if not href:
                    continue
                
                absolute_url = urljoin(base_url, href)
                
                # Si c'est un article, l'ajouter
                if self._is_article_url(absolute_url) and not self._is_category_page(absolute_url):
                    article_urls.add(absolute_url)
                # Si c'est une page de cat√©gorie, l'explorer r√©cursivement
                elif self._is_category_page(absolute_url) and absolute_url not in visited:
                    await crawl_page(absolute_url, depth + 1)
        
        except Exception as e:
            self.logger.debug("Heuristic crawl failed", url=url, error=str(e))
    
    # Explorer les pages candidates
    for candidate_url in blog_candidates:
        if len(article_urls) >= max_urls:
            break
        await crawl_page(candidate_url)
    
    return list(article_urls)[:max_urls]
```

**Estimation :** 1h30

---

### T√¢che 2.3 : Am√©liorer la d√©couverte RSS avec pagination

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Probl√®me :** Les RSS feeds ne sont pas pagin√©s.

**Solution :**
- [ ] D√©tecter et suivre les pages RSS suivantes (`?paged=2`, etc.)
- [ ] Limiter √† 3-5 pages maximum

**Modifications dans `_parse_rss_feed()` :**
```python
async def _parse_rss_feed(self, feed_url: str) -> List[str]:
    """
    Parse RSS feed and extract article URLs (with pagination support).
    
    Args:
        feed_url: RSS feed URL
        
    Returns:
        List of article URLs
    """
    all_urls = set()
    max_pages = 3  # Limiter √† 3 pages
    
    for page in range(1, max_pages + 1):
        if page > 1:
            # Essayer diff√©rentes formes de pagination
            paged_urls = [
                f"{feed_url}?paged={page}",
                f"{feed_url}?page={page}",
                f"{feed_url}/page/{page}/",
            ]
        else:
            paged_urls = [feed_url]
        
        found_urls = False
        for paged_url in paged_urls:
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.get(paged_url)
                    if response.status_code != 200:
                        continue
                    
                    soup = BeautifulSoup(response.text, "xml")
                    page_urls = []
                    
                    # RSS format
                    for item in soup.find_all("item"):
                        link = item.find("link")
                        if link and link.text:
                            page_urls.append(link.text.strip())
                    
                    # Atom format
                    for entry in soup.find_all("entry"):
                        link = entry.find("link")
                        if link:
                            href = link.get("href") or link.text
                            if href:
                                page_urls.append(href.strip())
                    
                    if page_urls:
                        all_urls.update(page_urls)
                        found_urls = True
                        self.logger.debug(
                            "RSS page parsed",
                            feed_url=paged_url,
                            urls_found=len(page_urls),
                        )
                        break  # Succ√®s, passer √† la page suivante
            
            except Exception as e:
                self.logger.debug("Failed to parse RSS page", feed_url=paged_url, error=str(e))
                continue
        
        # Si aucune URL trouv√©e sur cette page, arr√™ter
        if not found_urls:
            break
    
    return list(all_urls)
```

**Estimation :** 45 minutes

---

## üéØ Phase 3 : Am√©liorations avanc√©es (Priorit√© 3)

### T√¢che 3.1 : Ajouter un d√©tecteur d'articles bas√© sur le contenu HTML

**Fichier :** Nouveau fichier `python_scripts/ingestion/article_detector.py` (optionnel)

**Description :** Cr√©er un d√©tecteur qui analyse le contenu HTML pour d√©terminer si une page est un article.

**Fonctionnalit√©s :**
- [ ] D√©tecter la pr√©sence de balises `<article>`
- [ ] Analyser la structure HTML (classes communes : `post`, `entry`, `article-content`)
- [ ] Calculer un score de confiance bas√© sur :
  - Pr√©sence de titre (h1)
  - Longueur du contenu
  - Pr√©sence de date de publication
  - Ratio texte/HTML

**Utilisation :**
- Utiliser ce d√©tecteur en fallback si les patterns ne trouvent rien
- Filtrer les URLs candidates avant de les scraper

**Estimation :** 2h (optionnel, peut √™tre fait plus tard)

---

### T√¢che 3.2 : Am√©liorer la gestion des extensions exclues

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Modifications :**
- [ ] Cr√©er une constante `EXCLUDED_EXTENSIONS`
- [ ] Filtrer les URLs avec ces extensions dans toutes les strat√©gies

**Code :**
```python
EXCLUDED_EXTENSIONS = [
    '.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg',
    '.css', '.js', '.xml', '.zip', '.doc', '.docx',
    '.xls', '.xlsx', '.ppt', '.pptx', '.mp4', '.mp3',
]
```

**Estimation :** 20 minutes

---

### T√¢che 3.3 : Rendre le filtrage par √¢ge configurable

**Fichier :** `python_scripts/agents/agent_scraping.py`

**Modifications :**
- [ ] Ajouter un param√®tre `max_age_days` dans `__init__()`
- [ ] Permettre de d√©sactiver le filtrage par √¢ge (None = pas de limite)

**Code :**
```python
def __init__(self, min_word_count: int = 150, max_age_days: Optional[int] = 730) -> None:
    """Initialize the scraping agent."""
    super().__init__("scraping")
    self.min_word_count = min_word_count
    self.max_age_days = max_age_days  # None = pas de limite
```

**Estimation :** 15 minutes

---

## üìä R√©sum√© des t√¢ches

| Phase | T√¢che | Priorit√© | Impact | Temps estim√© |
|-------|-------|----------|--------|--------------|
| 1.1 | √âlargir les patterns | üî¥ CRITIQUE | 70-80% | 30 min |
| 1.2 | Fallback sitemap | üî¥ CRITIQUE | 50-60% | 45 min |
| 1.3 | D√©tection cat√©gories | üî¥ CRITIQUE | 20-30% | 30 min |
| 2.1 | Assouplir word count | üü° MOYEN | 20-30% | 15 min |
| 2.2 | Heuristics r√©cursives | üü° MOYEN | 15-25% | 1h30 |
| 2.3 | RSS pagination | üü° MOYEN | 10-15% | 45 min |
| 3.1 | D√©tecteur HTML | üü¢ FAIBLE | 5-10% | 2h (opt) |
| 3.2 | Extensions exclues | üü¢ FAIBLE | 5% | 20 min |
| 3.3 | √Çge configurable | üü¢ FAIBLE | 5-10% | 15 min |

**Temps total Phase 1 :** ~1h45  
**Temps total Phase 2 :** ~2h30  
**Temps total Phase 3 :** ~2h35 (optionnel)

---

## üöÄ Ordre d'ex√©cution recommand√©

1. **Phase 1 compl√®te** (T√¢ches 1.1, 1.2, 1.3) - Impact imm√©diat
2. **T√¢che 2.1** - Quick win (15 min)
3. **T√¢che 2.3** - Am√©lioration RSS (45 min)
4. **T√¢che 2.2** - Heuristics r√©cursives (1h30)
5. **Phase 3** - Si n√©cessaire apr√®s tests

---

## ‚úÖ Crit√®res de succ√®s

Apr√®s impl√©mentation, on devrait observer :
- ‚úÖ `total_articles_discovered > 0` pour la majorit√© des domaines
- ‚úÖ `total_articles_saved > 0` dans les logs
- ‚úÖ R√©duction significative de `domains_without_articles`
- ‚úÖ Statistiques d√©taill√©es montrant o√π les articles sont trouv√©s (sitemap/RSS/heuristics)

---

## üìù Notes d'impl√©mentation

1. **Tests apr√®s chaque phase :** Tester avec un petit √©chantillon de domaines avant de passer √† la phase suivante
2. **Logging :** Conserver les logs d√©taill√©s pour diagnostiquer les probl√®mes restants
3. **Performance :** Surveiller les temps d'ex√©cution, surtout avec les heuristics r√©cursives
4. **Compatibilit√© :** S'assurer que les modifications restent compatibles avec l'API existante

---

## üîÑ It√©rations futures possibles

- Cache des URLs d√©couvertes pour √©viter de re-scraper
- D√©tection automatique de nouveaux patterns bas√©e sur l'apprentissage
- Support de sitemaps index (sitemap_index.xml)
- Am√©lioration de l'extraction de contenu avec des s√©lecteurs CSS personnalis√©s

