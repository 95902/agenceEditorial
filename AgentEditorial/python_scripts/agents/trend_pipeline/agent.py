"""Trend Pipeline Agent - Orchestrates the 4-stage hybrid trend extraction pipeline."""

from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
from uuid import uuid4

import numpy as np
from sqlalchemy.ext.asyncio import AsyncSession

from python_scripts.agents.base_agent import BaseAgent
from python_scripts.agents.trend_pipeline.clustering import BertopicClusterer, ClusteringConfig, EmbeddingFetcher, OutlierHandler, TopicLabeler
from python_scripts.agents.trend_pipeline.temporal import TemporalAnalyzer, TemporalConfig
from python_scripts.agents.trend_pipeline.llm_enrichment import LLMEnricher, LLMEnrichmentConfig
from python_scripts.agents.trend_pipeline.gap_analysis import GapAnalyzer, GapAnalysisConfig
from python_scripts.database.crud_clusters import (
    create_topic_clusters_batch,
    create_topic_outliers_batch,
)
from python_scripts.database.models import TrendPipelineExecution
from python_scripts.utils.logging import get_logger
from python_scripts.analysis.article_enrichment.topic_filters import (
    classify_topic_label,
)

logger = get_logger(__name__)


class TrendPipelineAgent(BaseAgent):
    """
    Orchestrator for the 4-stage trend extraction pipeline.
    
    Stages:
    1. Clustering (BERTopic + HDBSCAN)
    2. Temporal Analysis
    3. LLM Validation & Enrichment
    4. Gap Analysis
    """
    
    def __init__(
        self,
        db_session: AsyncSession,
        clustering_config: Optional[ClusteringConfig] = None,
        temporal_config: Optional[TemporalConfig] = None,
        llm_config: Optional[LLMEnrichmentConfig] = None,
        gap_config: Optional[GapAnalysisConfig] = None,
        client_domain: Optional[str] = None,
    ):
        """
        Initialize the trend pipeline agent.
        
        Args:
            db_session: Database session
            clustering_config: Clustering configuration
            temporal_config: Temporal analysis configuration
            llm_config: LLM enrichment configuration
            gap_config: Gap analysis configuration
            client_domain: Client domain for generating collection name
        """
        super().__init__(agent_name="trend_pipeline")
        self.db_session = db_session
        self.client_domain = client_domain
        
        # Initialize configs with client_domain if provided
        if clustering_config is None:
            # Create config with client_domain to trigger __post_init__
            if client_domain:
                clustering_config = ClusteringConfig(client_domain=client_domain)
            else:
                clustering_config = ClusteringConfig.default()
        elif client_domain and not clustering_config.client_domain:
            # Recreate config with client_domain to trigger __post_init__
            # This ensures __post_init__ is called to generate collection name
            clustering_config = ClusteringConfig(
                hdbscan=clustering_config.hdbscan,
                umap=clustering_config.umap,
                bertopic=clustering_config.bertopic,
                use_qdrant_embeddings=clustering_config.use_qdrant_embeddings,
                embedding_collection=None,  # Will be generated by __post_init__
                client_domain=client_domain,
                client_collection=clustering_config.client_collection,
                normalize_embeddings=clustering_config.normalize_embeddings,
                min_articles=clustering_config.min_articles,
                max_age_days=clustering_config.max_age_days,
                save_outliers=clustering_config.save_outliers,
                analyze_outliers=clustering_config.analyze_outliers,
                max_outliers_to_analyze=clustering_config.max_outliers_to_analyze,
                generate_visualizations=clustering_config.generate_visualizations,
                save_centroids_to_qdrant=clustering_config.save_centroids_to_qdrant,
                centroid_collection=clustering_config.centroid_collection,
            )
        elif client_domain and clustering_config.embedding_collection == "competitor_articles":
            # If collection name is still default, regenerate it
            from python_scripts.vectorstore.qdrant_client import get_competitor_collection_name
            clustering_config.embedding_collection = get_competitor_collection_name(client_domain)
        
        self.clustering_config = clustering_config
        self.temporal_config = temporal_config or TemporalConfig.default()
        self.llm_config = llm_config or LLMEnrichmentConfig.default()
        self.gap_config = gap_config or GapAnalysisConfig.default()
        
        # Initialize analyzers
        self._clusterer = BertopicClusterer(self.clustering_config)
        self._embedding_fetcher = EmbeddingFetcher(self.clustering_config)
        self._outlier_handler = OutlierHandler(self.clustering_config)
        self._topic_labeler = TopicLabeler(self.clustering_config)
        self._temporal_analyzer = TemporalAnalyzer(self.temporal_config)
        self._llm_enricher = LLMEnricher(self.llm_config)
        self._gap_analyzer = GapAnalyzer(self.gap_config)
        
        # Log collection name for debugging
        if client_domain:
            logger.info(
                "Trend pipeline initialized with client domain",
                client_domain=client_domain,
                collection_name=self.clustering_config.embedding_collection,
            )

    async def _validate_prerequisites(self, domains: List[str]) -> Dict[str, Any]:
        """
        Validate prerequisites before running the pipeline.

        Args:
            domains: List of domains to analyze

        Returns:
            Validation result with success status and error message if applicable
        """
        collection_name = self.clustering_config.embedding_collection

        logger.info(
            "Validating pipeline prerequisites",
            collection=collection_name,
            domains=domains,
        )

        # Check if Qdrant collection exists
        try:
            client = self._embedding_fetcher.client
            collections = client.get_collections().collections
            collection_names = [c.name for c in collections]

            if collection_name not in collection_names:
                error_msg = (
                    f"Qdrant collection '{collection_name}' does not exist. "
                    f"Available collections: {collection_names}. "
                    "This indicates that no articles have been scraped yet for this client domain. "
                    "Please run the scraping pipeline first to index competitor articles."
                )
                logger.error(
                    "Prerequisites validation failed: collection not found",
                    collection=collection_name,
                    available_collections=collection_names,
                )
                return {"success": False, "error": error_msg}

            # Check collection is not empty
            try:
                collection_info = client.get_collection(collection_name)
                points_count = getattr(collection_info, "points_count", 0)

                if points_count == 0:
                    error_msg = (
                        f"Collection '{collection_name}' exists but contains no articles. "
                        "Please run the scraping pipeline to index competitor articles before running trend analysis."
                    )
                    logger.error(
                        "Prerequisites validation failed: empty collection",
                        collection=collection_name,
                        points_count=0,
                    )
                    return {"success": False, "error": error_msg}

                logger.info(
                    "Prerequisites validation passed",
                    collection=collection_name,
                    points_count=points_count,
                )
                return {"success": True, "points_count": points_count}

            except Exception as e:
                logger.warning(
                    "Could not get collection info during validation",
                    collection=collection_name,
                    error=str(e),
                )
                # Continue anyway - will be caught later in fetch_embeddings
                return {"success": True}

        except Exception as e:
            error_msg = f"Failed to validate prerequisites: {str(e)}"
            logger.error(
                "Prerequisites validation error",
                error=str(e),
            )
            return {"success": False, "error": error_msg}
    
    async def execute(
        self,
        domains: List[str],
        client_domain: Optional[str] = None,
        time_window_days: int = 365,
        skip_llm: bool = False,
        skip_gap_analysis: bool = False,
        execution_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Execute the full 4-stage pipeline.
        
        Args:
            domains: List of domains to analyze
            client_domain: Client domain for gap analysis
            time_window_days: Time window in days
            skip_llm: Skip LLM enrichment stage
            skip_gap_analysis: Skip gap analysis stage
            execution_id: Optional execution ID (if provided, uses this instead of generating a new one)
            
        Returns:
            Pipeline execution results
        """
        execution_id = execution_id or str(uuid4())
        start_time = datetime.now(timezone.utc)
        
        logger.info(
            "Starting trend pipeline",
            execution_id=execution_id,
            domains=domains,
            client_domain=client_domain,
        )
        
        # Create execution record
        execution = TrendPipelineExecution(
            execution_id=execution_id,
            client_domain=client_domain,
            domains_analyzed={"domains": domains},
            time_window_days=time_window_days,
        )
        self.db_session.add(execution)
        await self.db_session.commit()

        results = {
            "execution_id": execution_id,
            "stages": {},
            "success": True,
        }

        try:
            # Validate prerequisites before starting
            logger.info("Validating prerequisites before starting pipeline")
            validation_result = await self._validate_prerequisites(domains)

            if not validation_result.get("success"):
                error_msg = validation_result.get("error", "Prerequisites validation failed")
                execution.error_message = error_msg
                execution.stage_1_clustering_status = "failed"
                await self.db_session.commit()

                logger.error(
                    "Pipeline execution aborted due to failed prerequisites",
                    execution_id=execution_id,
                    error=error_msg,
                )

                results["success"] = False
                results["error"] = error_msg
                return results

            # STAGE 1: Clustering
            logger.info("Starting Stage 1: Clustering")
            execution.stage_1_clustering_status = "in_progress"
            await self.db_session.commit()
            
            stage1_result = await self._execute_stage_1_clustering(
                domains=domains,
                time_window_days=time_window_days,
            )
            
            results["stages"]["clustering"] = stage1_result
            
            if not stage1_result.get("success"):
                execution.stage_1_clustering_status = "failed"
                execution.error_message = stage1_result.get("error")
                await self.db_session.commit()
                results["success"] = False
                return results
            
            execution.stage_1_clustering_status = "completed"
            execution.total_clusters = len(stage1_result.get("clusters", []))
            execution.total_outliers = len(stage1_result.get("outliers", []))
            execution.total_articles = stage1_result.get("total_articles", 0)
            await self.db_session.commit()
            
            # Save clusters to database
            if stage1_result.get("clusters"):
                await create_topic_clusters_batch(
                    self.db_session,
                    analysis_id=execution.id,
                    clusters_data=stage1_result["clusters"],
                )
            
            if stage1_result.get("outliers"):
                await create_topic_outliers_batch(
                    self.db_session,
                    analysis_id=execution.id,
                    outliers_data=stage1_result["outliers"],
                )
            
            # STAGE 2: Temporal Analysis
            logger.info("Starting Stage 2: Temporal Analysis")
            execution.stage_2_temporal_status = "in_progress"
            await self.db_session.commit()
            
            stage2_result = await self._execute_stage_2_temporal(
                clusters=stage1_result["clusters"],
                documents=stage1_result.get("documents", []),
                topics=stage1_result.get("topics", []),
                embeddings=stage1_result.get("embeddings"),
                centroids=stage1_result.get("centroids"),
            )
            
            results["stages"]["temporal"] = stage2_result
            execution.stage_2_temporal_status = "completed"
            await self.db_session.commit()
            
            # Save temporal metrics to database
            if stage2_result.get("metrics"):
                from python_scripts.database.crud_temporal_metrics import create_topic_temporal_metrics_batch
                try:
                    await create_topic_temporal_metrics_batch(
                        db_session=self.db_session,
                        metrics_data=stage2_result["metrics"],
                        analysis_id=execution.id,
                    )
                    logger.info(
                        "Saved temporal metrics to database",
                        metrics_count=len(stage2_result["metrics"]),
                        analysis_id=execution.id,
                    )
                except Exception as e:
                    logger.warning(
                        "Failed to save temporal metrics",
                        error=str(e),
                        analysis_id=execution.id,
                    )
            
            # STAGE 3: LLM Enrichment
            if not skip_llm:
                logger.info("Starting Stage 3: LLM Enrichment")
                execution.stage_3_llm_status = "in_progress"
                await self.db_session.commit()
                
                stage3_result = await self._execute_stage_3_llm(
                    analysis_id=execution.id,
                    clusters=stage1_result["clusters"],
                    temporal_metrics=stage2_result.get("metrics", []),
                    outliers=stage1_result.get("outliers", []),
                    texts=stage1_result.get("texts", []),
                )
                
                results["stages"]["llm_enrichment"] = stage3_result
                execution.stage_3_llm_status = "completed"
                execution.total_recommendations = len(stage3_result.get("recommendations", []))
                await self.db_session.commit()
            else:
                execution.stage_3_llm_status = "skipped"
                await self.db_session.commit()
            
            # STAGE 4: Gap Analysis
            if not skip_gap_analysis and client_domain:
                logger.info("Starting Stage 4: Gap Analysis")
                execution.stage_4_gap_status = "in_progress"
                await self.db_session.commit()
                
                stage4_result = await self._execute_stage_4_gap_analysis(
                    analysis_id=execution.id,
                    client_domain=client_domain,
                    clusters=stage1_result["clusters"],
                    documents=stage1_result.get("documents", []),
                    topics=stage1_result.get("topics", []),
                    temporal_metrics=stage2_result.get("metrics", []),
                    recommendations=results["stages"].get("llm_enrichment", {}).get("recommendations", []),
                )
                
                results["stages"]["gap_analysis"] = stage4_result
                execution.stage_4_gap_status = "completed"
                execution.total_gaps = len(stage4_result.get("gaps", []))
                await self.db_session.commit()
            else:
                execution.stage_4_gap_status = "skipped"
                await self.db_session.commit()
            
            # Finalize
            end_time = datetime.now(timezone.utc)
            execution.end_time = end_time
            execution.duration_seconds = int((end_time - start_time).total_seconds())
            await self.db_session.commit()
            
            results["duration_seconds"] = execution.duration_seconds
            
            logger.info(
                "Trend pipeline completed",
                execution_id=execution_id,
                duration=execution.duration_seconds,
                clusters=execution.total_clusters,
                gaps=execution.total_gaps,
            )
            
            return results
            
        except Exception as e:
            logger.error("Pipeline failed", error=str(e))
            execution.error_message = str(e)
            await self.db_session.commit()
            
            results["success"] = False
            results["error"] = str(e)
            return results
    
    async def _execute_stage_1_clustering(
        self,
        domains: List[str],
        time_window_days: int,
    ) -> Dict[str, Any]:
        """Execute Stage 1: Clustering."""
        # Fetch embeddings from Qdrant
        embeddings, metadata, document_ids = self._embedding_fetcher.fetch_embeddings(
            domains=domains,
            max_age_days=time_window_days,
        )

        if len(embeddings) < self.clustering_config.min_articles:
            error_msg = (
                f"Not enough articles ({len(embeddings)}). Minimum: {self.clustering_config.min_articles}. "
                f"Collection: {self.clustering_config.embedding_collection}. "
            )

            if len(embeddings) == 0:
                error_msg += (
                    "No articles were found in the collection. "
                    "This typically means the scraping pipeline has not been run yet, "
                    "or articles were scraped with a different client_domain configuration. "
                    "Please run the scraping pipeline first to index competitor articles."
                )
            else:
                error_msg += (
                    f"The collection contains {len(embeddings)} articles, but at least "
                    f"{self.clustering_config.min_articles} are required for meaningful clustering. "
                    "Run the scraping pipeline to index more articles, or adjust the min_articles threshold."
                )

            logger.error(
                "Stage 1 clustering prerequisites not met",
                articles_found=len(embeddings),
                articles_required=self.clustering_config.min_articles,
                collection=self.clustering_config.embedding_collection,
                domains_requested=domains,
            )

            return {
                "success": False,
                "error": error_msg,
            }
        
        # Extract texts for clustering
        texts = [m.get("content_text", m.get("title", "")) for m in metadata]
        
        # Run clustering
        cluster_result = self._clusterer.cluster(
            texts=texts,
            embeddings=embeddings,
            metadata=metadata,
        )
        
        if not cluster_result.get("success"):
            return cluster_result
        
        # Generate labels
        clusters = self._topic_labeler.generate_labels(cluster_result["clusters"])
        clusters = self._topic_labeler.calculate_coherence_scores(
            clusters,
            embeddings=embeddings,
            topics=cluster_result["topics"],
        )
        
        # Prepare clusters for database
        for cluster in clusters:
            cluster["document_ids"] = {
                "indices": cluster.get("document_indices", []),
                "ids": [
                    document_ids[i]
                    for i in cluster.get("document_indices", [])
                    if i < len(document_ids)
                ],
            }
            cluster["top_terms"] = {"terms": cluster.get("top_terms", [])}

            # Classify topic scope (core / adjacent / off_scope)
            label = cluster.get("label", "") or ""
            scope = classify_topic_label(label)
            cluster["scope"] = scope
        
        # Process outliers
        outliers = self._outlier_handler.extract_outliers(
            topics=cluster_result["topics"],
            metadata=metadata,
            document_ids=document_ids,
            embeddings=embeddings,
            centroids=cluster_result.get("centroids"),
        )
        
        # Categorize outliers
        self._outlier_handler.categorize_outliers(outliers, texts)
        
        # Prepare outliers for database
        for outlier in outliers:
            outlier["embedding_distance"] = outlier.get("distance_to_nearest")
        
        # Prepare documents with index
        documents = []
        for i, (meta, doc_id) in enumerate(zip(metadata, document_ids)):
            doc = dict(meta)
            doc["index"] = i
            doc["document_id"] = doc_id
            documents.append(doc)
        
        return {
            "success": True,
            "clusters": clusters,
            "outliers": outliers,
            "topics": cluster_result["topics"],
            "embeddings": embeddings,
            "centroids": cluster_result.get("centroids"),
            "documents": documents,
            "texts": texts,
            "total_articles": len(embeddings),
        }
    
    async def _execute_stage_2_temporal(
        self,
        clusters: List[Dict[str, Any]],
        documents: List[Dict[str, Any]],
        topics: List[int],
        embeddings: Optional[np.ndarray],
        centroids: Optional[Dict[int, np.ndarray]],
    ) -> Dict[str, Any]:
        """Execute Stage 2: Temporal Analysis."""
        # Group documents by topic
        documents_by_topic = {}
        for doc, topic_id in zip(documents, topics):
            if topic_id < 0:
                continue
            if topic_id not in documents_by_topic:
                documents_by_topic[topic_id] = []
            documents_by_topic[topic_id].append(doc)
        
        # Analyze temporal metrics
        metrics = self._temporal_analyzer.analyze_all_topics(
            clusters=clusters,
            documents_by_topic=documents_by_topic,
            centroids=centroids,
            embeddings=embeddings,
        )
        
        # Detect topics over time
        topics_over_time = self._temporal_analyzer.detect_topics_over_time(
            documents=documents,
            topics=topics,
        )
        
        return {
            "success": True,
            "metrics": metrics,
            "topics_over_time": topics_over_time,
        }
    
    async def _execute_stage_3_llm(
        self,
        analysis_id: int,
        clusters: List[Dict[str, Any]],
        temporal_metrics: List[Dict[str, Any]],
        outliers: List[Dict[str, Any]],
        texts: List[str],
    ) -> Dict[str, Any]:
        """Execute Stage 3: LLM Enrichment."""
        from python_scripts.database.crud_llm_results import (
            create_trend_analysis,
            create_article_recommendation,
        )
        from python_scripts.database.crud_clusters import get_topic_cluster_by_topic_id
        
        # Build temporal lookup
        temporal_lookup = {m["topic_id"]: m for m in temporal_metrics}
        
        syntheses = []
        recommendations = []
        
        # Process top topics by potential score
        # Utiliser la config max_topics_to_enrich au lieu du hardcode Ã  10
        max_topics = getattr(self.llm_config, "max_topics_to_enrich", 50)
        top_topics = sorted(
            temporal_metrics,
            key=lambda x: x.get("potential_score", 0),
            reverse=True,
        )[:max_topics]
        
        logger.info(
            "Processing topics for LLM enrichment",
            total_topics=len(temporal_metrics),
            topics_to_process=len(top_topics),
            max_topics_config=max_topics,
        )
        
        for metrics in top_topics:
            topic_id = metrics["topic_id"]
            
            # Find cluster
            cluster = next((c for c in clusters if c["topic_id"] == topic_id), None)
            if not cluster:
                continue
            
            # Get database cluster record
            db_cluster = await get_topic_cluster_by_topic_id(
                self.db_session,
                analysis_id,
                topic_id,
            )
            if not db_cluster:
                logger.warning(f"Database cluster not found for topic {topic_id}, analysis_id={analysis_id}")
                continue
            
            # Extract keywords
            keywords = [t["word"] for t in cluster.get("top_terms", {}).get("terms", [])[:10]]
            
            # Get representative docs
            doc_indices = cluster.get("document_ids", {}).get("indices", [])[:3]
            rep_docs = [texts[i] for i in doc_indices if i < len(texts)]
            
            try:
                # Generate synthesis
                synthesis = await self._llm_enricher.synthesize_trend(
                    topic_label=cluster["label"],
                    keywords=keywords,
                    volume=cluster["size"],
                    time_period=365,
                    velocity=metrics.get("velocity", 1.0),
                    velocity_trend=metrics.get("velocity_trend", "stable"),
                    source_diversity=metrics.get("source_diversity", 1),
                    representative_docs=rep_docs,
                )
                
                synthesis["topic_id"] = topic_id
                syntheses.append(synthesis)
                
                # Save trend analysis to database
                try:
                    await create_trend_analysis(
                        db_session=self.db_session,
                        topic_cluster_id=db_cluster.id,
                        synthesis=synthesis.get("synthesis", ""),
                        saturated_angles=synthesis.get("saturated_angles"),
                        opportunities=synthesis.get("opportunities"),
                        llm_model_used=synthesis.get("llm_model_used", "unknown"),
                    )
                except Exception as e:
                    logger.warning(f"Failed to save trend analysis for topic {topic_id}", error=str(e))
                
                # Generate article angles
                angles = await self._llm_enricher.generate_article_angles(
                    topic_label=cluster["label"],
                    keywords=keywords,
                    saturated_angles=synthesis.get("saturated_angles", []),
                    opportunities=synthesis.get("opportunities", []),
                    num_angles=3,
                )
                
                for angle in angles:
                    angle["topic_cluster_id"] = topic_id
                    recommendations.append(angle)
                    
                    # Save article recommendation to database
                    try:
                        await create_article_recommendation(
                            db_session=self.db_session,
                            topic_cluster_id=db_cluster.id,
                            title=angle.get("title", ""),
                            hook=angle.get("hook", ""),
                            outline=angle.get("outline", {}),
                            effort_level=angle.get("effort_level", "medium"),
                            differentiation_score=angle.get("differentiation_score"),
                        )
                    except Exception as e:
                        logger.warning(f"Failed to save article recommendation for topic {topic_id}", error=str(e))
                    
            except Exception as e:
                logger.warning(f"LLM enrichment failed for topic {topic_id}", error=str(e))
        
        # Analyze outliers for weak signals
        weak_signal_analysis = None
        if outliers:
            try:
                weak_signal_analysis = await self._llm_enricher.analyze_outliers(
                    outliers=outliers,
                    texts=texts,
                )
            except Exception as e:
                logger.warning("Outlier analysis failed", error=str(e))
        
        return {
            "success": True,
            "syntheses": syntheses,
            "recommendations": recommendations,
            "weak_signal_analysis": weak_signal_analysis,
        }
    
    async def _execute_stage_4_gap_analysis(
        self,
        analysis_id: int,
        client_domain: str,
        clusters: List[Dict[str, Any]],
        documents: List[Dict[str, Any]],
        topics: List[int],
        temporal_metrics: List[Dict[str, Any]],
        recommendations: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Execute Stage 4: Gap Analysis."""
        from python_scripts.database.crud_gaps import (
            create_editorial_gap,
            create_content_roadmap_item,
        )
        from python_scripts.database.crud_clusters import get_topic_cluster_by_topic_id
        from python_scripts.database.crud_llm_results import get_article_recommendations_by_topic_cluster
        
        # Group documents by topic
        documents_by_topic = {}
        for doc, topic_id in zip(documents, topics):
            if topic_id < 0:
                continue
            if topic_id not in documents_by_topic:
                documents_by_topic[topic_id] = []
            documents_by_topic[topic_id].append(doc)
        
        # Analyze coverage
        coverage = self._gap_analyzer.analyze_coverage(
            client_domain=client_domain,
            clusters=clusters,
            documents_by_topic=documents_by_topic,
        )
        
        # Save coverage analysis to database
        if coverage:
            from python_scripts.database.crud_coverage import create_client_coverage_analysis_batch
            try:
                await create_client_coverage_analysis_batch(
                    db_session=self.db_session,
                    client_domain=client_domain,
                    coverage_data=coverage,
                    analysis_id=analysis_id,
                )
                logger.info(
                    "Saved client coverage analyses to database",
                    coverage_count=len(coverage),
                    analysis_id=analysis_id,
                )
            except Exception as e:
                logger.warning(
                    "Failed to save coverage analyses",
                    error=str(e),
                    analysis_id=analysis_id,
                )
        
        # Identify gaps
        gaps = self._gap_analyzer.identify_gaps(
            coverage_results=coverage,
            temporal_metrics=temporal_metrics,
        )
        
        # Save gaps to database
        gap_id_map = {}  # Map topic_id -> gap_id
        for gap_data in gaps:
            topic_id = gap_data["topic_id"]
            
            # Get database cluster record
            db_cluster = await get_topic_cluster_by_topic_id(
                self.db_session,
                analysis_id,
                topic_id,
            )
            if not db_cluster:
                logger.warning(f"Database cluster not found for topic {topic_id} in gap analysis")
                continue
            
            try:
                gap = await create_editorial_gap(
                    db_session=self.db_session,
                    client_domain=client_domain,
                    topic_cluster_id=db_cluster.id,
                    coverage_score=gap_data["coverage_score"],
                    priority_score=gap_data["priority_score"],
                    diagnostic=gap_data["diagnostic"],
                    opportunity_description=gap_data["opportunity_description"],
                    risk_assessment=gap_data["risk_assessment"],
                )
                gap_id_map[topic_id] = gap.id
            except Exception as e:
                logger.warning(f"Failed to save gap for topic {topic_id}", error=str(e))
        
        # Identify strengths
        strengths = self._gap_analyzer.identify_strengths(coverage)
        
        # Save client strengths to database
        if strengths:
            from python_scripts.database.crud_coverage import create_client_strengths_batch
            try:
                await create_client_strengths_batch(
                    db_session=self.db_session,
                    client_domain=client_domain,
                    strengths_data=strengths,
                    analysis_id=analysis_id,
                )
                logger.info(
                    "Saved client strengths to database",
                    strengths_count=len(strengths),
                    analysis_id=analysis_id,
                )
            except Exception as e:
                logger.warning(
                    "Failed to save client strengths",
                    error=str(e),
                    analysis_id=analysis_id,
                )
        
        # Build roadmap
        roadmap = self._gap_analyzer.build_roadmap(
            gaps=gaps,
            recommendations=recommendations,
        )
        
        # Save roadmap to database
        for roadmap_item in roadmap:
            gap_topic_id = roadmap_item.get("gap_id")  # This is topic_id from gap
            recommendation_title = roadmap_item.get("recommendation_title", "")
            
            if gap_topic_id not in gap_id_map:
                continue
            
            gap_id = gap_id_map[gap_topic_id]
            
            # Find matching article recommendation
            db_cluster = await get_topic_cluster_by_topic_id(
                self.db_session,
                analysis_id,
                gap_topic_id,
            )
            if not db_cluster:
                continue
            
            # Get article recommendations for this topic
            article_recos = await get_article_recommendations_by_topic_cluster(
                self.db_session,
                db_cluster.id,
            )
            
            # Find matching recommendation by title
            matching_reco = None
            for reco in article_recos:
                if reco.title == recommendation_title or recommendation_title in reco.title:
                    matching_reco = reco
                    break
            
            # If no exact match, use first recommendation
            if not matching_reco and article_recos:
                matching_reco = article_recos[0]
            
            if matching_reco:
                try:
                    await create_content_roadmap_item(
                        db_session=self.db_session,
                        client_domain=client_domain,
                        gap_id=gap_id,
                        recommendation_id=matching_reco.id,
                        priority_order=roadmap_item.get("priority_order", 999),
                        estimated_effort=roadmap_item.get("estimated_effort", "medium"),
                        status="pending",  # Roadmap items start as pending
                    )
                except Exception as e:
                    logger.warning(f"Failed to save roadmap item for topic {gap_topic_id}", error=str(e))
        
        return {
            "success": True,
            "coverage": coverage,
            "gaps": gaps,
            "strengths": strengths,
            "roadmap": roadmap,
        }

