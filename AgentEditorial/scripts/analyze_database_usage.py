#!/usr/bin/env python3
"""Analyse complÃ¨te de l'utilisation de la base de donnÃ©es aprÃ¨s le workflow complet.

Ce script :
1. Compte les lignes dans chaque table
2. VÃ©rifie l'utilisation dans le code
3. Identifie pourquoi certaines tables sont vides
4. Liste les tables non utilisÃ©es ou obsolÃ¨tes
"""

import asyncio
import re
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from collections import defaultdict

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import text, inspect
from sqlalchemy.ext.asyncio import AsyncSession

from python_scripts.database.db_session import AsyncSessionLocal
from python_scripts.database.models import (
    AuditLog,
    ClientArticle,
    CompetitorArticle,
    ContentRoadmap,
    CrawlCache,
    ClientCoverageAnalysis,
    ClientStrength,
    DiscoveryLog,
    EditorialGap,
    ErrorLog,
    ArticleRecommendation,
    PerformanceMetric,
    ScrapingPermission,
    SiteAnalysisResult,
    SiteDiscoveryProfile,
    SiteProfile,
    TopicCluster,
    TopicOutlier,
    TopicTemporalMetrics,
    TrendAnalysis,
    TrendPipelineExecution,
    UrlDiscoveryScore,
    WeakSignalAnalysis,
    WorkflowExecution,
    GeneratedArticle,
    GeneratedArticleImage,
    GeneratedArticleVersion,
    GeneratedImage,
)


# Mapping table_name -> Model class
TABLE_MODEL_MAP = {
    "site_profiles": SiteProfile,
    "workflow_executions": WorkflowExecution,
    "site_analysis_results": SiteAnalysisResult,
    "competitor_articles": CompetitorArticle,
    "client_articles": ClientArticle,
    "topic_clusters": TopicCluster,
    "topic_outliers": TopicOutlier,
    "topic_temporal_metrics": TopicTemporalMetrics,
    "trend_analysis": TrendAnalysis,
    "article_recommendations": ArticleRecommendation,
    "weak_signals_analysis": WeakSignalAnalysis,
    "client_coverage_analysis": ClientCoverageAnalysis,
    "editorial_gaps": EditorialGap,
    "client_strengths": ClientStrength,
    "content_roadmap": ContentRoadmap,
    "trend_pipeline_executions": TrendPipelineExecution,
    "crawl_cache": CrawlCache,
    "scraping_permissions": ScrapingPermission,
    "performance_metrics": PerformanceMetric,
    "audit_log": AuditLog,
    "site_discovery_profiles": SiteDiscoveryProfile,
    "url_discovery_scores": UrlDiscoveryScore,
    "discovery_logs": DiscoveryLog,
    "error_logs": ErrorLog,
    "generated_articles": GeneratedArticle,
    "generated_article_images": GeneratedArticleImage,
    "generated_article_versions": GeneratedArticleVersion,
    "generated_images": GeneratedImage,
}


async def count_table_rows(db: AsyncSession, table_name: str) -> int:
    """Compte les lignes dans une table."""
    try:
        result = await db.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
        return result.scalar_one() or 0
    except Exception as e:
        return -1  # Erreur (table n'existe peut-Ãªtre pas)


async def get_table_size(db: AsyncSession, table_name: str) -> str:
    """RÃ©cupÃ¨re la taille d'une table."""
    try:
        result = await db.execute(
            text(
                f"SELECT pg_size_pretty(pg_total_relation_size('public.{table_name}'))"
            )
        )
        return result.scalar_one() or "0 kB"
    except Exception:
        return "unknown"


def find_code_references(table_name: str, model_class_name: str, codebase_path: Path) -> Dict[str, List[str]]:
    """Trouve les rÃ©fÃ©rences Ã  une table dans le code."""
    references = {
        "imports": [],
        "crud_usage": [],
        "api_routes": [],
        "agents": [],
        "direct_sql": [],
    }
    
    model_pattern = re.compile(rf'\b{re.escape(model_class_name)}\b')
    table_pattern = re.compile(rf'\b{re.escape(table_name)}\b', re.IGNORECASE)
    
    for file_path in codebase_path.rglob("*.py"):
        # Ignorer certains dossiers
        if any(skip in str(file_path) for skip in ['__pycache__', '.git', '.venv', 'node_modules', '.cursor', 'migrations']):
            continue
        
        try:
            content = file_path.read_text(encoding='utf-8')
            rel_path = str(file_path.relative_to(codebase_path))
        except Exception:
            continue
        
        # Imports
        if model_pattern.search(content) and 'from python_scripts.database.models import' in content:
            if rel_path not in references["imports"]:
                references["imports"].append(rel_path)
        
        # CRUD usage
        if 'crud' in rel_path.lower() and model_pattern.search(content):
            if rel_path not in references["crud_usage"]:
                references["crud_usage"].append(rel_path)
        
        # API routes
        if 'api/routers' in rel_path and model_pattern.search(content):
            if rel_path not in references["api_routes"]:
                references["api_routes"].append(rel_path)
        
        # Agents
        if 'agents' in rel_path and model_pattern.search(content):
            if rel_path not in references["agents"]:
                references["agents"].append(rel_path)
        
        # SQL direct
        if table_pattern.search(content) and any(kw in content for kw in ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'FROM']):
            if rel_path not in references["direct_sql"]:
                references["direct_sql"].append(rel_path)
    
    return references


def get_table_purpose(table_name: str) -> str:
    """Retourne le but/usage attendu d'une table."""
    purposes = {
        "site_profiles": "Profils Ã©ditoriaux des sites clients analysÃ©s",
        "workflow_executions": "Suivi des exÃ©cutions de workflows (sites, competitors, discovery, etc.)",
        "site_analysis_results": "RÃ©sultats dÃ©taillÃ©s par phase de l'analyse Ã©ditoriale",
        "competitor_articles": "Articles scrapÃ©s des sites concurrents",
        "client_articles": "Articles scrapÃ©s du site client",
        "topic_clusters": "Clusters thÃ©matiques crÃ©Ã©s par BERTopic (Stage 1 Trend Pipeline)",
        "topic_outliers": "Articles non classifiÃ©s par BERTopic (outliers)",
        "topic_temporal_metrics": "MÃ©triques temporelles par cluster (Stage 2 Trend Pipeline)",
        "trend_analysis": "SynthÃ¨ses LLM des tendances par cluster (Stage 3 Trend Pipeline)",
        "article_recommendations": "Recommandations d'articles gÃ©nÃ©rÃ©es par LLM (Stage 3)",
        "weak_signals_analysis": "Analyse des signaux faibles (outliers groupÃ©s)",
        "client_coverage_analysis": "Analyse de couverture client par topic (Stage 4)",
        "editorial_gaps": "Gaps Ã©ditoriaux identifiÃ©s (Stage 4)",
        "client_strengths": "Forces compÃ©titives du client (Stage 4)",
        "content_roadmap": "Roadmap de contenu priorisÃ©e (Stage 4)",
        "trend_pipeline_executions": "Suivi des exÃ©cutions du Trend Pipeline",
        "crawl_cache": "Cache des pages crawlÃ© pour Ã©viter les re-scraping",
        "scraping_permissions": "Cache des permissions robots.txt par domaine",
        "performance_metrics": "MÃ©triques de performance des workflows",
        "audit_log": "Logs d'audit des actions des agents",
        "site_discovery_profiles": "Profils de dÃ©couverte optimisÃ©s par domaine",
        "url_discovery_scores": "Scores de probabilitÃ© pour les URLs dÃ©couvertes",
        "discovery_logs": "Logs des opÃ©rations de dÃ©couverte",
        "error_logs": "Logs d'erreurs pour diagnostic",
        "generated_articles": "Articles gÃ©nÃ©rÃ©s par le pipeline de gÃ©nÃ©ration",
        "generated_article_images": "Images gÃ©nÃ©rÃ©es pour les articles",
        "generated_article_versions": "Versions historiques des articles gÃ©nÃ©rÃ©s",
        "generated_images": "Images gÃ©nÃ©rÃ©es avec Z-Image (standalone)",
    }
    return purposes.get(table_name, "Usage non documentÃ©")


async def analyze_all_tables() -> Dict[str, Dict]:
    """Analyse toutes les tables."""
    results = {}
    codebase_path = Path(__file__).parent.parent
    
    async with AsyncSessionLocal() as db:
        print("ğŸ“Š Analyse de la base de donnÃ©es...\n")
        
        for table_name, model_class in TABLE_MODEL_MAP.items():
            print(f"  Analysant {table_name}...")
            
            # Compter les lignes
            row_count = await count_table_rows(db, table_name)
            size = await get_table_size(db, table_name)
            
            # Trouver les rÃ©fÃ©rences dans le code
            model_class_name = model_class.__name__
            references = find_code_references(table_name, model_class_name, codebase_path)
            
            # Calculer le score d'utilisation
            usage_score = (
                len(references["imports"]) * 2 +
                len(references["crud_usage"]) * 3 +
                len(references["api_routes"]) * 3 +
                len(references["agents"]) * 2 +
                len(references["direct_sql"]) * 1
            )
            
            # DÃ©terminer le statut
            is_used = usage_score > 0
            has_data = row_count > 0
            is_empty = row_count == 0
            
            results[table_name] = {
                "row_count": row_count,
                "size": size,
                "has_data": has_data,
                "is_empty": is_empty,
                "references": references,
                "usage_score": usage_score,
                "is_used": is_used,
                "model_class": model_class_name,
                "purpose": get_table_purpose(table_name),
            }
    
    return results


def generate_report(results: Dict[str, Dict], output_path: Path) -> None:
    """GÃ©nÃ¨re un rapport markdown complet."""
    
    # CatÃ©goriser les tables
    filled_and_used = []
    filled_but_unused = []
    empty_but_used = []
    empty_and_unused = []
    
    for table_name, data in results.items():
        has_data = data["has_data"]
        is_used = data["is_used"]
        
        if has_data and is_used:
            filled_and_used.append((table_name, data))
        elif has_data and not is_used:
            filled_but_unused.append((table_name, data))
        elif not has_data and is_used:
            empty_but_used.append((table_name, data))
        else:
            empty_and_unused.append((table_name, data))
    
    report = []
    report.append("# Analyse complÃ¨te de la base de donnÃ©es aprÃ¨s workflow\n\n")
    report.append(f"**Date d'analyse** : {Path(__file__).stat().st_mtime}\n\n")
    
    # RÃ©sumÃ©
    report.append("## ğŸ“Š RÃ©sumÃ© exÃ©cutif\n\n")
    report.append(f"- **Total de tables analysÃ©es** : {len(results)}\n")
    report.append(f"- **Tables remplies et utilisÃ©es** : {len(filled_and_used)} âœ…\n")
    report.append(f"- **Tables remplies mais non utilisÃ©es** : {len(filled_but_unused)} âš ï¸\n")
    report.append(f"- **Tables vides mais utilisÃ©es** : {len(empty_but_used)} âš ï¸\n")
    report.append(f"- **Tables vides et non utilisÃ©es** : {len(empty_and_unused)} âŒ\n\n")
    
    # Section 1: Tables remplies et utilisÃ©es
    report.append("## âœ… 1. Tables remplies et utilisÃ©es\n\n")
    report.append("Ces tables contiennent des donnÃ©es et sont utilisÃ©es dans le code.\n\n")
    report.append("| Table | Lignes | Taille | Usage | But |\n")
    report.append("|-------|--------|--------|-------|-----|\n")
    
    for table_name, data in sorted(filled_and_used, key=lambda x: x[1]["row_count"], reverse=True):
        usage_count = sum(len(files) for files in data["references"].values())
        report.append(
            f"| `{table_name}` | {data['row_count']} | {data['size']} | {usage_count} refs | {data['purpose'][:50]}... |\n"
        )
    report.append("\n")
    
    # Section 2: Tables remplies mais non utilisÃ©es
    if filled_but_unused:
        report.append("## âš ï¸ 2. Tables remplies mais non utilisÃ©es\n\n")
        report.append("Ces tables contiennent des donnÃ©es mais ne sont pas rÃ©fÃ©rencÃ©es dans le code.\n\n")
        report.append("| Table | Lignes | Taille | Raison probable |\n")
        report.append("|-------|--------|--------|------------------|\n")
        
        for table_name, data in sorted(filled_but_unused, key=lambda x: x[1]["row_count"], reverse=True):
            reason = "Table obsolÃ¨te ou donnÃ©es historiques"
            report.append(f"| `{table_name}` | {data['row_count']} | {data['size']} | {reason} |\n")
        report.append("\n")
    
    # Section 3: Tables vides mais utilisÃ©es
    if empty_but_used:
        report.append("## âš ï¸ 3. Tables vides mais utilisÃ©es dans le code\n\n")
        report.append("Ces tables sont rÃ©fÃ©rencÃ©es dans le code mais sont vides. Raisons possibles :\n\n")
        report.append("| Table | Usage | Raison probable |\n")
        report.append("|-------|-------|------------------|\n")
        
        for table_name, data in sorted(empty_but_used):
            usage_count = sum(len(files) for files in data["references"].values())
            reason = "Workflow non exÃ©cutÃ© ou Ã©tape sautÃ©e"
            if "trend" in table_name.lower():
                reason = "Trend Pipeline non exÃ©cutÃ© ou Ã©tape spÃ©cifique sautÃ©e"
            elif "generated" in table_name.lower():
                reason = "GÃ©nÃ©ration d'article non effectuÃ©e"
            elif "discovery" in table_name.lower():
                reason = "Discovery/Scraping non effectuÃ©"
            
            report.append(f"| `{table_name}` | {usage_count} refs | {reason} |\n")
        report.append("\n")
    
    # Section 4: Tables vides et non utilisÃ©es
    if empty_and_unused:
        report.append("## âŒ 4. Tables vides et non utilisÃ©es\n\n")
        report.append("Ces tables sont vides et ne sont pas rÃ©fÃ©rencÃ©es dans le code.\n\n")
        report.append("| Table | But | Action recommandÃ©e |\n")
        report.append("|-------|-----|-------------------|\n")
        
        for table_name, data in sorted(empty_and_unused):
            action = "VÃ©rifier si nÃ©cessaire, sinon supprimer"
            report.append(f"| `{table_name}` | {data['purpose']} | {action} |\n")
        report.append("\n")
    
    # Section 5: DÃ©tails par table
    report.append("## ğŸ“‹ 5. DÃ©tails complets par table\n\n")
    
    for table_name, data in sorted(results.items()):
        report.append(f"### `{table_name}`\n\n")
        report.append(f"- **But** : {data['purpose']}\n")
        report.append(f"- **Lignes** : {data['row_count']}\n")
        report.append(f"- **Taille** : {data['size']}\n")
        report.append(f"- **ModÃ¨le** : `{data['model_class']}`\n")
        report.append(f"- **Score d'utilisation** : {data['usage_score']}\n")
        
        if data['references']:
            report.append(f"- **RÃ©fÃ©rences dans le code** :\n")
            for ref_type, files in data['references'].items():
                if files:
                    report.append(f"  - **{ref_type}** : {len(files)} fichier(s)\n")
                    for file in files[:3]:
                        report.append(f"    - `{file}`\n")
                    if len(files) > 3:
                        report.append(f"    - ... et {len(files) - 3} autre(s)\n")
        else:
            report.append(f"- **RÃ©fÃ©rences** : Aucune\n")
        
        report.append("\n")
    
    # Section 6: Recommandations
    report.append("## ğŸ’¡ 6. Recommandations\n\n")
    
    if empty_and_unused:
        report.append("### Tables Ã  supprimer\n\n")
        report.append("Les tables suivantes sont vides et non utilisÃ©es. Elles peuvent Ãªtre supprimÃ©es :\n\n")
        for table_name, data in sorted(empty_and_unused):
            report.append(f"- `{table_name}` : {data['purpose']}\n")
        report.append("\n")
    
    if empty_but_used:
        report.append("### Tables Ã  vÃ©rifier\n\n")
        report.append("Les tables suivantes sont utilisÃ©es mais vides. VÃ©rifier si le workflow correspondant a Ã©tÃ© exÃ©cutÃ© :\n\n")
        for table_name, data in sorted(empty_but_used):
            report.append(f"- `{table_name}` : {data['purpose']}\n")
        report.append("\n")
    
    if filled_but_unused:
        report.append("### Tables Ã  nettoyer\n\n")
        report.append("Les tables suivantes contiennent des donnÃ©es mais ne sont pas utilisÃ©es. VÃ©rifier si elles sont obsolÃ¨tes :\n\n")
        for table_name, data in sorted(filled_but_unused, key=lambda x: x[1]["row_count"], reverse=True):
            report.append(f"- `{table_name}` : {data['row_count']} lignes - {data['purpose']}\n")
        report.append("\n")
    
    # Ã‰crire le rapport
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(''.join(report))
    
    print(f"\nâœ… Rapport gÃ©nÃ©rÃ© : {output_path}")


async def main():
    """Point d'entrÃ©e principal."""
    results = await analyze_all_tables()
    
    # GÃ©nÃ©rer le rapport
    output_path = Path(__file__).parent.parent / "ANALYSE_DATABASE_USAGE.md"
    generate_report(results, output_path)
    
    # Afficher un rÃ©sumÃ© dans la console
    print("\n" + "=" * 80)
    print("RÃ‰SUMÃ‰ DE L'ANALYSE")
    print("=" * 80)
    
    filled_count = sum(1 for r in results.values() if r["has_data"])
    used_count = sum(1 for r in results.values() if r["is_used"])
    empty_unused = sum(1 for r in results.values() if not r["has_data"] and not r["is_used"])
    
    print(f"\nğŸ“Š Statistiques :")
    print(f"   - Tables avec donnÃ©es : {filled_count}/{len(results)}")
    print(f"   - Tables utilisÃ©es : {used_count}/{len(results)}")
    print(f"   - Tables vides et non utilisÃ©es : {empty_unused}/{len(results)}")
    
    print(f"\nğŸ“„ Rapport complet : {output_path}")
    print("=" * 80)


if __name__ == "__main__":
    asyncio.run(main())

