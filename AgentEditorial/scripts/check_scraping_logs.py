"""Script pour v√©rifier les logs de scraping r√©cents."""

import asyncio
import json
import sys
from datetime import datetime, timedelta
from typing import Optional

from sqlalchemy import select, desc
from sqlalchemy.ext.asyncio import AsyncSession

# Ajouter le chemin du projet
sys.path.insert(0, str(__file__).replace("/scripts/check_scraping_logs.py", ""))

from python_scripts.database.db_session import AsyncSessionLocal
from python_scripts.database.models import WorkflowExecution


def format_duration(seconds: Optional[int]) -> str:
    """Format duration in human-readable format."""
    if not seconds:
        return "N/A"
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    secs = seconds % 60
    if hours > 0:
        return f"{hours}h {minutes}m {secs}s"
    elif minutes > 0:
        return f"{minutes}m {secs}s"
    else:
        return f"{secs}s"


def format_statistics(stats: dict) -> str:
    """Format statistics dictionary."""
    if not stats:
        return "Aucune statistique"
    
    lines = []
    for key, value in stats.items():
        if isinstance(value, dict):
            lines.append(f"  {key}:")
            for sub_key, sub_value in value.items():
                lines.append(f"    {sub_key}: {sub_value}")
        else:
            lines.append(f"  {key}: {value}")
    return "\n".join(lines)


async def check_recent_scraping_logs(limit: int = 5, execution_id: Optional[str] = None):
    """
    V√©rifier les logs de scraping r√©cents.
    
    Args:
        limit: Nombre d'ex√©cutions √† afficher
        execution_id: ID d'ex√©cution sp√©cifique (optionnel)
    """
    async with AsyncSessionLocal() as session:
        try:
            # Construire la requ√™te
            query = select(WorkflowExecution).where(
                WorkflowExecution.workflow_type == "scraping",
                WorkflowExecution.is_valid == True,  # noqa: E712
            )
            
            if execution_id:
                query = query.where(WorkflowExecution.execution_id == execution_id)
            else:
                query = query.order_by(desc(WorkflowExecution.created_at)).limit(limit)
            
            result = await session.execute(query)
            executions = result.scalars().all()
            
            if not executions:
                print("‚ùå Aucune ex√©cution de scraping trouv√©e.")
                return
            
            print(f"\n{'='*80}")
            print(f"üìä LOGS DE SCRAPING ({len(executions)} ex√©cution(s))")
            print(f"{'='*80}\n")
            
            for i, execution in enumerate(executions, 1):
                print(f"{'‚îÄ'*80}")
                print(f"üìã Ex√©cution #{i}")
                print(f"{'‚îÄ'*80}")
                print(f"üÜî Execution ID: {execution.execution_id}")
                print(f"üìÖ Cr√©√© le: {execution.created_at}")
                print(f"‚è±Ô∏è  Dur√©e: {format_duration(execution.duration_seconds)}")
                print(f"üìä Statut: {execution.status}")
                print(f"‚úÖ Succ√®s: {'Oui' if execution.was_success else 'Non'}")
                
                if execution.start_time:
                    print(f"üïê D√©but: {execution.start_time}")
                if execution.end_time:
                    print(f"üïê Fin: {execution.end_time}")
                
                # Input data
                if execution.input_data:
                    print(f"\nüì• INPUT DATA:")
                    input_data = execution.input_data
                    domains = input_data.get("domains", [])
                    client_domain = input_data.get("client_domain")
                    max_articles = input_data.get("max_articles_per_domain", 100)
                    
                    if client_domain:
                        print(f"  Client Domain: {client_domain}")
                    if domains:
                        print(f"  Domaines: {len(domains)} domaines")
                        if len(domains) <= 10:
                            for domain in domains:
                                print(f"    - {domain}")
                        else:
                            for domain in domains[:5]:
                                print(f"    - {domain}")
                            print(f"    ... et {len(domains) - 5} autres")
                    print(f"  Max articles par domaine: {max_articles}")
                
                # Output data
                if execution.output_data:
                    print(f"\nüì§ OUTPUT DATA:")
                    output_data = execution.output_data
                    
                    # Statistiques globales
                    stats = output_data.get("statistics", {})
                    if stats:
                        print(f"\nüìä STATISTIQUES GLOBALES:")
                        print(f"  Total domaines: {stats.get('total_domains', 0)}")
                        print(f"  Domaines avec articles d√©couverts: {stats.get('domains_with_articles_discovered', 0)}")
                        print(f"  Domaines sans articles: {stats.get('domains_without_articles', 0)}")
                        print(f"  Domaines avec erreurs: {stats.get('domains_with_errors', 0)}")
                        print(f"  Total articles d√©couverts: {stats.get('total_articles_discovered', 0)}")
                        print(f"  Total articles sauvegard√©s: {stats.get('total_articles_saved', 0)}")
                        print(f"  Articles d√©j√† existants: {stats.get('total_articles_already_exists', 0)}")
                        print(f"  Articles √©chou√©s (crawl): {stats.get('total_articles_crawl_failed', 0)}")
                        print(f"  Articles filtr√©s: {stats.get('total_articles_filtered', 0)}")
                        print(f"  Erreurs: {stats.get('total_articles_errors', 0)}")
                    
                    # Articles par domaine
                    articles_by_domain = output_data.get("articles_by_domain", {})
                    total_scraped = output_data.get("total_articles_scraped", 0)
                    
                    print(f"\nüì∞ ARTICLES PAR DOMAINE:")
                    print(f"  Total articles scrap√©s: {total_scraped}")
                    
                    if articles_by_domain:
                        domains_with_articles = {
                            domain: len(articles)
                            for domain, articles in articles_by_domain.items()
                            if articles
                        }
                        
                        if domains_with_articles:
                            print(f"  Domaines avec articles ({len(domains_with_articles)}):")
                            # Trier par nombre d'articles d√©croissant
                            sorted_domains = sorted(
                                domains_with_articles.items(),
                                key=lambda x: x[1],
                                reverse=True
                            )
                            for domain, count in sorted_domains[:10]:
                                print(f"    - {domain}: {count} article(s)")
                            if len(sorted_domains) > 10:
                                print(f"    ... et {len(sorted_domains) - 10} autres domaines")
                        else:
                            print(f"  ‚ö†Ô∏è  Aucun domaine n'a d'articles scrap√©s")
                    else:
                        print(f"  ‚ö†Ô∏è  Aucun article trouv√©")
                
                # Error message
                if execution.error_message:
                    print(f"\n‚ùå ERREUR:")
                    print(f"  {execution.error_message}")
                
                print()
            
            print(f"{'='*80}\n")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration des logs: {e}")
            import traceback
            traceback.print_exc()


async def main():
    """Main function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="V√©rifier les logs de scraping")
    parser.add_argument(
        "-n",
        "--limit",
        type=int,
        default=5,
        help="Nombre d'ex√©cutions √† afficher (d√©faut: 5)",
    )
    parser.add_argument(
        "-e",
        "--execution-id",
        type=str,
        help="ID d'ex√©cution sp√©cifique √† afficher",
    )
    
    args = parser.parse_args()
    
    await check_recent_scraping_logs(limit=args.limit, execution_id=args.execution_id)


if __name__ == "__main__":
    asyncio.run(main())

