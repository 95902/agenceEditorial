#!/usr/bin/env python3
"""
Analyse des concurrents : nombre trouvÃ©s vs nombre scrapÃ©s

Ce script :
1. RÃ©cupÃ¨re la liste complÃ¨te des concurrents trouvÃ©s
2. VÃ©rifie combien ont Ã©tÃ© scrapÃ©s (ont des articles)
3. Affiche les statistiques dÃ©taillÃ©es
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import select, desc, func
from sqlalchemy.ext.asyncio import AsyncSession

from python_scripts.database.db_session import AsyncSessionLocal
from python_scripts.database.models import WorkflowExecution, CompetitorArticle
from python_scripts.database.crud_articles import count_competitor_articles


async def analyze_competitors_scraping(domain: str = "innosys.fr"):
    """Analyse les concurrents trouvÃ©s et scrapÃ©s."""
    async with AsyncSessionLocal() as db:
        # 1. RÃ©cupÃ©rer la derniÃ¨re exÃ©cution de recherche de concurrents
        stmt = (
            select(WorkflowExecution)
            .where(
                WorkflowExecution.workflow_type == "competitor_search",
                WorkflowExecution.status == "completed",
                WorkflowExecution.input_data["domain"].astext == domain,
            )
            .order_by(desc(WorkflowExecution.start_time))
            .limit(1)
        )
        
        result = await db.execute(stmt)
        execution = result.scalar_one_or_none()
        
        if not execution or not execution.output_data:
            print(f"âŒ Aucune recherche de concurrents trouvÃ©e pour {domain}")
            return
        
        competitors_data = execution.output_data.get("competitors", [])
        all_candidates = execution.output_data.get("all_candidates", [])
        excluded_candidates = execution.output_data.get("excluded_candidates", [])
        
        print("="*80)
        print(f"ANALYSE DES CONCURRENTS - {domain}")
        print("="*80)
        print(f"\nğŸ“Š STATISTIQUES GÃ‰NÃ‰RALES")
        print("-"*80)
        
        # Nombre total de candidats trouvÃ©s
        total_found = execution.output_data.get("total_found", len(all_candidates) if all_candidates else len(competitors_data))
        print(f"Total de candidats trouvÃ©s: {total_found}")
        
        # Nombre de concurrents inclus
        total_included = len(competitors_data)
        print(f"Concurrents inclus (final): {total_included}")
        
        # Nombre de concurrents exclus
        total_excluded = len(excluded_candidates) if excluded_candidates else 0
        print(f"Concurrents exclus: {total_excluded}")
        
        # Concurrents validÃ©s
        validated_competitors = [
            c for c in competitors_data
            if c.get("validated", False) or c.get("manual", False)
        ]
        print(f"Concurrents validÃ©s: {len(validated_competitors)}")
        
        # Concurrents non exclus
        non_excluded = [
            c for c in competitors_data
            if not c.get("excluded", False)
        ]
        print(f"Concurrents non exclus: {len(non_excluded)}")
        
        # 2. VÃ©rifier combien ont Ã©tÃ© scrapÃ©s
        print(f"\nğŸ“° ANALYSE DU SCRAPING")
        print("-"*80)
        
        # Utiliser les concurrents validÃ©s ou tous les non exclus
        competitors_to_check = validated_competitors if validated_competitors else non_excluded
        
        scraped_count = 0
        not_scraped = []
        scraping_stats = []
        
        for competitor in competitors_to_check:
            comp_domain = competitor.get("domain")
            if not comp_domain:
                continue
            
            # Compter les articles pour ce concurrent
            article_count = await count_competitor_articles(db, domain=comp_domain)
            
            if article_count > 0:
                scraped_count += 1
                scraping_stats.append({
                    "domain": comp_domain,
                    "articles": article_count,
                    "validated": competitor.get("validated", False),
                    "similarity": competitor.get("relevance_score", 0) * 100 if competitor.get("relevance_score") else 0
                })
            else:
                not_scraped.append({
                    "domain": comp_domain,
                    "validated": competitor.get("validated", False),
                    "similarity": competitor.get("relevance_score", 0) * 100 if competitor.get("relevance_score") else 0
                })
        
        print(f"Concurrents Ã  vÃ©rifier: {len(competitors_to_check)}")
        print(f"âœ… Concurrents scrapÃ©s (avec articles): {scraped_count}")
        print(f"âŒ Concurrents non scrapÃ©s (sans articles): {len(not_scraped)}")
        
        # Statistiques sur les articles
        total_articles = sum(s["articles"] for s in scraping_stats)
        avg_articles = total_articles / scraped_count if scraped_count > 0 else 0
        
        print(f"\nğŸ“ˆ STATISTIQUES DES ARTICLES")
        print("-"*80)
        print(f"Total d'articles scrapÃ©s: {total_articles}")
        print(f"Moyenne d'articles par concurrent scrapÃ©: {avg_articles:.1f}")
        
        if scraping_stats:
            max_articles = max(s["articles"] for s in scraping_stats)
            min_articles = min(s["articles"] for s in scraping_stats)
            print(f"Min articles: {min_articles}, Max articles: {max_articles}")
        
        # DÃ©tail des concurrents scrapÃ©s
        if scraping_stats:
            print(f"\nâœ… CONCURRENTS SCRAPÃ‰S ({scraped_count})")
            print("-"*80)
            # Trier par nombre d'articles dÃ©croissant
            scraping_stats.sort(key=lambda x: x["articles"], reverse=True)
            for i, stat in enumerate(scraping_stats, 1):
                validated_marker = "âœ“" if stat["validated"] else " "
                print(f"{i}. {stat['domain']}")
                print(f"   Articles: {stat['articles']} | SimilaritÃ©: {stat['similarity']:.0f}% | ValidÃ©: {validated_marker}")
        
        # DÃ©tail des concurrents non scrapÃ©s
        if not_scraped:
            print(f"\nâŒ CONCURRENTS NON SCRAPÃ‰S ({len(not_scraped)})")
            print("-"*80)
            for i, comp in enumerate(not_scraped, 1):
                validated_marker = "âœ“" if comp["validated"] else " "
                print(f"{i}. {comp['domain']}")
                print(f"   SimilaritÃ©: {comp['similarity']:.0f}% | ValidÃ©: {validated_marker}")
        
        # RÃ©sumÃ© final
        print(f"\n{'='*80}")
        print("RÃ‰SUMÃ‰")
        print(f"{'='*80}")
        print(f"ğŸ“Š Candidats trouvÃ©s: {total_found}")
        print(f"âœ… Concurrents inclus: {total_included}")
        print(f"   â””â”€ ValidÃ©s: {len(validated_competitors)}")
        print(f"   â””â”€ Non exclus: {len(non_excluded)}")
        print(f"ğŸ“° Concurrents scrapÃ©s: {scraped_count}/{len(competitors_to_check)} ({scraped_count/len(competitors_to_check)*100:.1f}%)" if competitors_to_check else "ğŸ“° Concurrents scrapÃ©s: 0/0")
        print(f"ğŸ“„ Total articles scrapÃ©s: {total_articles}")
        
        if competitors_to_check and scraped_count < len(competitors_to_check):
            print(f"\nâš ï¸ {len(not_scraped)} concurrent(s) n'ont pas encore Ã©tÃ© scrapÃ©s.")
            print("   Lancer le scraping pour ces concurrents pour enrichir les donnÃ©es.")
        
        print("="*80)


async def main():
    """Point d'entrÃ©e principal."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Analyse des concurrents trouvÃ©s vs scrapÃ©s"
    )
    parser.add_argument(
        "--domain",
        type=str,
        default="innosys.fr",
        help="Domaine Ã  analyser (dÃ©faut: innosys.fr)"
    )
    
    args = parser.parse_args()
    
    await analyze_competitors_scraping(args.domain)


if __name__ == "__main__":
    asyncio.run(main())


